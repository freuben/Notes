\hypertarget{chapter7}{}
\chapter{Compositions}

This chapter aims to briefly describe the portfolio of submitted work. However, this chapter focuses exclusively on technical and musical descriptions of the musical output and does not have as its purpose to be a commentary on the background, motivation or theoretical framework of the creative process (the previous chapters already serve as a meta-commentary of the submitted work). I will therefore concentrate on . . . .

%During the research period, I have engaged in work that is varied in its outcome: it includes results that maybe are difficult to categorize, but which involve a varying amount of pre-compositional work. Some of my creative output therefore may be categorized as compositions, improvisations, performances, studies or spontaneous experiments.

\section{E-tudes}

\emph{E-tudes}\footnote{See \hyperlink{portfolio}{Contents of Portfolio}: CD I (\emph{Compositions}), tracks 1--4 and DVD I (\emph{E-tudes}).} is a set of electronic \emph{\'{e}tudes} for six stage pianos, live electronics and mechanical piano.\footnote{In case a mechanical piano is not available, it is possible to use a sampler with piano sounds.} These compositions were written for the ensemble \textbf{piano}\emph{circus}\footnote{See \href{http://www.pianocircus.com/}{\texttt{http://www.pianocircus.com/}}} for a project that became a \mbox{two-year} collaboration and lead to two performances.\footnote{Enterprise 08 Festival, The Space, London, May, 2008, and The Sound Source, Kings Place, London, July, 2009.} What initially attracted me to this ensemble was its very particular instrumentation consisting of six electronic stage pianos. I thought this would be a suitable platform to experiment with the notions of \emph{real-time plunderphonics} and \emph{live musica derivata},\footnote{See \hyperlink{realtimeplunderfuck}{pp. 91--92}.} considering that these instruments are electronic and therefore produce no considerable audible acoustic sound.\footnote{The only acoustic sounds that can be heard are the keyclicks produced by the physical contact with the stage pianos while playing. This noise is slightly audible mostly when there are no sounds playing through the speakers (or they are very quiet).} Like a book of \emph{\'{e}tudes} from the repertoire, \emph{E-tudes} consists of a set of pieces that can be performed together at the same event or individually as separate short pieces. At present time, I have completed four `e-tudes', and as an ongoing project, I will continue adding new pieces to the collection. \emph{E-tudes} is modular in the way in which it can be presented: depending on the set of circumstances for a given event, they can be presented separately or as a whole, either as a concert performance or as an installation with perforative elements. In the installation version, the audience walks into, out of, and around the area surrounding the musicians and has creative control over how they want to experience the performance. By choosing between listening to the speakers in the room or to various headphones that are distributed through the performance space and generate different outputs, each member of the audience fabricates their own version of the piece. Therefore, in the installation version their are various possible outputs generated by the computer from the performance, which the audience can choose from. It is also possible to have a performance were the members of the audience are wearing wireless headphones that can receive multiple channels that are transmitted in the performance space, therefore allowing them to choose which channel they want to listen to.\footnote{This was the case in the performance at Kings Place.}

I use the same configuration for all of the pieces that comprise \emph{E-tudes}: the ensemble of six stage pianos is placed in hexagonal formation and divided into two subgroups. The first subgroup consisting of three pianists are asked to select \emph{\'{e}tudes} from the western piano repertoire at will---they can select the \emph{\'{e}tudes} they prefer to perform (for example, \emph{\'{e}tudes} by Chopin, Debussy or Ligeti, to mention just a few)---and are to play them in their chosen order during the duration of the performance. The second subgroup consisting of the remaining three pianists perform together from \emph{The Sixth Book of Madrigals} by Don Carlo Gesualdo da Venosa (1566-1613). 

The pianists playing the madrigals send Midi information to a computer that transforms the audio signal from the \emph{\'{e}tudes} and schedules the digital signal processing events. The audience is not be able to hear in the room what the pianists are playing as the stage pianos do not produce an acoustic sound. The seventh performer (performing the live-electronic part)\footnote{In the performances of \emph{E-tudes} I performed this part myself.} performs different tasks: at some points s/he speaks the Madrigals' text into a microphone and the spectral information from this signal is be used to process the final audio output and to trigger other sound events, at the same time playing Midi controllers. The live electronic part is not fixed, leaving space for improvisational elements within the human/computer interaction. Finally, through the analysis of all the inputs the computer sends Midi messages to the mechanical piano, adding yet another element to the performance. In the room the final result of the creative process of combining the simultaneous performances in diverse arrangements is diffused through the speakers. In the installation version, the headphones that are spread through the performance space portray the inner life of the performance sounding in the room and reveal the inner layers of computer processing and the appropriated compositions.

Computer programmes play a vital role in all the elements of \emph{E-tudes} and were written in SuperCollider---some of these prgrammes are discussed in \hyperlink{chapter5}{Chapter 5} but some were exclusively written for \emph{E-tudes}.\footnote{The code for these computer programmes can be found at \href{http://github.com/freuben/Etudes}{\texttt {http://github.com/freuben/Etudes}}.} These programmes are used to analyze incoming Midi data to schedule events\footnote{See \hyperlink{miditrig}{p. 113}.} and for digital signal processing (DSP). The incoming Midi events from the pianists playing Gesualdo is analyzed and divided by each voice of the original madrigals. The computer analysis using score following techniques tracks each voice and according to its position in the score, schedules specific DSP events. The Midi \emph{note} and \emph{velocity} information in some occasions is used to determine certain parameters in the DSP algorithms. The DSP algorithms of the live electronics use as input two mayor audio sources: the input of the combined live audio of the sound generated by the three pianists playing \emph{\'{e}tudes} and \emph{micro} elements\footnote{See \hyperlink{macroplunder}{pp. 89--90}.} derived from various recordings of existing music which I choose to appropriate. The individual live audio signals coming from each pianist playing \emph{\'{e}tudes} are interpolated with one another (by altering the pitch and volume of the signals).\footnote{Each signal is interpolated with the other by gradually pitch-shifting one signal down four octaves and fading out its volume gradually, while at the same time introducing the next signal which would be pitch-shifted four octaves down and gradually transposing it up until its normal pitch, and by gradually fading it in.} The live electronics performer can change the duration of the interpolation between \emph{\'{e}tudes} with a Midi controller. At the same time, the resulting signal is then pitch-shifted again through several pitch ratios (the original signal results in five different signals with varying pitch) generating multiple signals that are then mixed together to create yet another signal. The sounding result of this last signal is a very noisy signal which could be described as `piano noise' (it still retains a piano-like quality). I then utilize this `piano noise' as input in synthesis algorithms which filter it using several techniques. The `piano noise' however is very different to \emph{white noise}, \emph{pink noise} or any other types of noise used in classic synthesis techniques in that its spectral flux is constantly changing and its rate and amount of change is fairly irregular. Additionally, the live electronics performer can change the sonic qualities of the `piano noise'---and therefore also change its spectral flux---by altering the interpolation time of the live signals coming from the pianists playing \emph{\'{e}tudes}. The the first two `e-tudes' in their final result (the final output diffused through the speakers) are composed exclusively using synthesis algorithms which use this `piano noise' as input. At the same time, in the installation version, the audience can listen through headphones to the different outputs at different degrees of processing---for instance, the output of one of the headphone outputs is made out of material generated from the interpolation of \emph{\'{e}tudes}, while another one reveals the `piano noise'. The original appropriated sources (the \emph{\'{e}tudes} and the madrigals by Gesualdo) are also displayed closer to their original form through certain headphone outputs. The algorithms that control the overall process have also generative elements---each time they are performed, they generate slightly different results. The generative characteristics of the algorithms, the varying incoming data from the live performances (the \emph{\'{e}tudes} chosen by the pianists change for each performance of \emph{E-tudes}\footnote{Like Cage's \emph{Imaginary Landscape No.4}, this type of musical appropriation is current, generative and indeterminate. See \hyperlink{landscape4}{p. 81.}, for a further discussion of Cage's appropriation strategy.} and the incoming Midi data from the madrigals varies each time they are performed) as well as the improvisatory elements in the live electronic performer's part, makes \emph{E-tudes} an electronic composition that changes (both in content and performance) each time that it is performed, however maintaining certain elements that identify it as the same composition.

\emph{E-tude I} is based on the first madrigal of Gesualdo's \emph{Sixth Book of Madrigals} called \emph{Se la mia morte brami}. In \emph{E-tude I}, `piano noise' is filtered in different ways through the live electronics performer's voice reading the words of the madrigal. At the beginning of the piece, the noise is filtered through two vocoders. I appropriated the computer code\footnote{See \hyperlink{codeapprop}{p. 92}, for a discussion on the potential of appropriating computer code for as a musical strategy.} of a classic vocoder and altered it so that the \emph{bandwidth} and \emph{center frequency} of each band modulates differently for each band.\footnote{I modulated the \emph{bandwidth} with low frequency noise signals (to add irregularity to the filtered frequency range) and the \emph{center frequency} with sine waves (to add a slight vibrato to each band).} At the same time the two vocoders detune their fundamental frequency gradually away from the frequency initially specified at the beginning of the piece. One vocoder gradually detunes up, while the other detunes down until after approximately two minutes, they reach the interval of a \emph{5th} between each other and then start detuning on the opposite direction, until their fundamental frequency merges again into the same frequency (after approximately one more minute). During this time, other sounds are introduced which are all generated from the same `piano noise'. However there are different types of sounds that are generated through different filtering techniques (spectral gating,\footnote{FFT technique which ignores the frequency bins which have magnitudes bellow a certain threshold.} bandpass filtering, short percussive noise envelopes, the FFTFilter described in the previous chapter,\footnote{See \hyperlink{fftfilter}{pp. 98--99}.} etc.). The \emph{frequency range} and \emph{center frequencies} that are used filter the `piano noise' at the same time are derived from the Midi note information received from the pianists performing the madrigal (the frequencies are derived from the pitch material tuned in \emph{just intonation}). The dynamics for these sounds are also shaped by the Midi velocity from the performance of the Gesualdo. At the same time, the sounds are diffused through different layers to five speakers which correspond to each voice of the Gesualdo. These layers also follow the madrigals phrase structure---the layers start and end at the same point in time were the madrigal's phrases do.\footnote{I use this technique in many of my compositions: I plunder the phrase structure of an existing composition to generate a blueprint for a new composition. See \hyperlink{macroplunder}{pp. 89--90}. I also wrote a computer application that automizes this process and generates a visual representation of phrase structure using a Midi File as an input. See \hyperlink{scorevisual}{pp. 109--112}.} After approximately three minutes, the music starts to gradually transform: First, the mechanical piano starts playing notes on the lower register, triggered by Midi information resulting from an FFT analysis of a specified frequency range of `piano noise'---the frequencies with the strongest magnitudes in that range are mapped to Midi notes, and their magnitudes to velocities. New pitched sounds emerge, generated through \emph{comb-filtering} techniques using the `piano noise' as input and their pitch is derived from the original Geusaldo. Later, the type of vocal filtering starts changing: the vocoder sounds are no longer audible, FFTFilter becomes more prominent (and more responding to the live performers voice) and the percussive sounds are now trigged through onset detection of the microphone input. High frequency pitched sounds emerge (filtering of `piano noise' through a bank of resonators) which at the same time have characteristics of bowed string harmonics (by plundering the fundamental frequency of a recording of violins playing harmonics to control the pitch modulation of the high frequency sounds). The music gradually becomes more rhythmic and pitched. Sounds with characteristics close to a celesta (a bank of band-pass filters with specified frequencies derived from the harmonic structure of a recorded celesta sound is used to filter the `piano noise') start playing pitch material derived from a combination of the Gesualdo madrigal and data that is derived (\emph{synthrumentaized})\footnote{See \hypertarget{spectactics}{pp. 76--77}.} from the `piano noise' signal. A rhythmic pattern starts emerging (performed by the live electronics performer) as well as new percussive sounds (filtering of different frequencies of `piano noise' to match those of traditional percussion instruments). The piano notes become more periodic and more synchronized with the rhythmic pattern emerging sounding more like a bass line. A prominent melody emerges which is driven by the FFTFilter, filtering `piano noise' and following the melodic contour of a plundered recording of a vocal sound---the melody is also modified by Midi note and velocity data. The character of the music at this point becomes more reminiscent of a song (or aria) with primitive rhythms. The `primitive song' ends after the melody disappears, the percussive rhythms strip down and the high frequency `bowed string' sounds continue sounding on their own for a few seconds.

\emph{E-tude II} uses \emph{Belt\`{a} poi che t'assenti}, the second madrigal of the same book of madrigals, as the control structure for the composition. In this `e-tude' the role of the `piano noise' is reduced only to be used as noise within synthesis algorithms, which create sounds reminiscent to instrumental sounds. The high frequency `bowed string' sound which ended \emph{E-tude I} is the first sound to be revealed, this time however modulating much more in pitch and gliding through a variation of the main melody of the `primitive song' in \emph{E-tude I}. The main high frequency melody is accompanied by two other `string harmonic' sounds which together generate harmonies which derive pitch material from the Gesualdo original. At the same time, these high frequency sounds and are accompanied by sounds reminiscent of plucked strings, which are generated through a variation of the Karplus-Strong plucked-string algorithm using a burst of `piano noise' as its excitation. The resulting pitch, however is modulated with the fundamental pitch of the analysis of a recording of a plucked \emph{Geomungo} (korean instrument) string. The amplitude envelope of the \emph{Geomungo} recording is also mapped to the result of the Karplus-Strong algorithm. These plucked-string sounds are triggered at specific points through the analysis of the incoming Midi data from the pianists playing Gesualdo, at some points also triggering other sounds that are characteristic of other Korean percussion instruments (these sounds are generated by using the `piano noise' as the input to the algorithms which generate the percussive sounds). At the same time, the mechanical piano is activated ones in a while by the incoming Midi data from the madrigal and plays an ascending \emph{arpeggio} of the \emph{synthrumentation} of spectral data that the microphone signal detects at that specific moment in time.\footnote{The live electronics performer at that moment focuses on producing vowel sounds with his/her voice to generate relatively pitched sounds. The PartialTracker class is used to detect the strongest frequencies of the vowel sounds, see \hyperlink{partrack}{p. 97}.} The notes of both the `plucked string' sounds as well as the \emph{arpeggios} are at the same time modified, using the Midi note information of the Gesualdo original. After approximately two minutes, the music stops and after a brief silence, a new sound is heard. This sound is generated through a combination of `classic' filtering techniques (band pass, high pass, dynamic bank of resonators, etc.) which filter the `piano noise' to try to emulate a `wind instrument' with vocal characteristics as well. The attack and release envelopes of this sound were therefore designed to sound close to a `recorder' or some sort of `flute', and the release of the sound sometimes comes with a high-frequency noise reminiscent of human breathing (the frequency range of the noise was mapped approximately to the frequency range of a person's breathing-in sound). This `virtual instrument' plays a melody that is plundered from a recording of \emph{Gagok}\footnote{\emph{Gagok} is a type of traditional korean court music. bla bla, reference...} and is modified in pitch by the Midi notes from the Gesualdo. After a brief solo section, we hear an `ensemble of virtual instruments' starts playing together music that sounds closer to \emph{Gagok}. All the `virtual instruments' at this point resemble korean instruments (except for the mechanical piano)---all of which use the `piano noise' as input to physical model algorithms. Two of the `virtual instruments' join in to play the same melody (one of them sounds more `sinusoidal' and the other sound closer to a reed instrument), but with slight melodic deviations in timing and tuning. These deviations are driven by an algorithm that is generative, meaning that each time that \emph{E-tude II} is performed, the melodic deviations vary. The music however, is not suggestive of \emph{Gagok} in two main features: the mechanical piano (which keeps playing the `spectral' \emph{arpeggios}) and the pitch material is different from traditional \emph{Gagok} in that it is modified by the incoming Midi data from the Gesualdo madrigal and therefore for example there are hints of intervalic relationships based on just intonation. At the same time, as the `\emph{Gagok} music' continues, the rate at which the sporadic events (plucked strings, percussion instruments and mechanical piano \emph{arpeggios}) happen, gradually becomes faster and the events themselves become more active and unpredictable (for instance the mechanical piano \emph{arpeggios} become faster and with more notes and their direction starts changing randomly near to the end). The events start occurring progressively faster and the music seems to be accelerating and becoming more and more dense until the music abruptly stops.

\emph{E-tude III} is based on the third madrigal by Gesualdo called \emph{Tu piangi, o filli mia}. This `e-tude' starts with a sinusoidal sound comprised of two pitches which gradually change in frequency using the same detuning trajectory as the vocoders do in \emph{E-tude I}---first they detune away from each other until they reach the interval of a \emph{5th} and then they move towards each other until they merge again. This sounds at the same time also follow the phrase structure of the madrigal. Just after hearing the first sinusoidal sound, the same `celesta' sounds that were used in \emph{E-tude I} (which are generated from filtered `piano noise') emerge. However, the pitch material that the `celesta' sounds generate is derived from the spectral data (\emph{synthrumentation}) of the audio signal of just one of the pianists playing \emph{\'{e}tudes}. The rhythm is triggered through onset detection of the live microphone signal (the voice of the live electronics performer reading the text of the madrigal). After approximately fifty seconds, the live electronics performer triggers percussive sounds with a Midi controller at periodic intervals. These sounds are produced by combining the information from the magnitudes of an FFT analysis of plundered recordings of different types of footsteps (different types of shoes and surfaces) with the information from the phases an FFT of the `piano noise'. After some time, a high frequency noise becomes audible and later some low frequency noise also surfaces---these noises come from selected bins of an FFT of the `piano noise'. Later, a single descending \emph{arpeggio} of `plucked string' sounds (generated through the same Karplus-Strong algorithm variation used in \emph{E-tude II}, but a plundered recording of a harp instead of a \emph{Geomungo}) reminiscent of the mechanical piano \emph{arpeggios} in \emph{E-tude II} announces the merging of the two sinusoidal sounds, which produce `beating'\footnote{} as they approximate to each other. After this event, both the celesta and sinusoidal sounds stop and new sounds coming from a random selection of FFT bins of the `piano noise' start to emerge. This gradual `spectral crescendo'---where the spectrum is gradually filled by random bins---culminates as the `piano noise' is revealed in its unfiltered and unprocessed form for the first time. The `piano noise' becomes more dominant and louder, covering the complete frequency spectrum as it is diffused equally over the speakers, until it vanishes completely in a matter of seconds through the reverse FFT process---the randomly selected FFT bins disappear one after the other until the `piano noise' `evaporates' into silence. A long period of silence follows. 

%\subsection*{E-tude IV}

%- Relate it to other chapters (interpassivity)


%	E-tudes questions the traditional role and relationships between performer, composer and listener and gives a unique and innovative approach to the use of “found objects”. The composer in this piece does not communicate with the performers by writing a score or by teaching them the music ‘by ear’ as in previous performance practice conventions. He even lets the performers decide which pieces to play within a given repertoire. Therefore, the creative role of the composer is not to provide the music the performer should play but rather, in Oswaldian terms, to plunder their audio signal. On the other hand, E-tudes differentiates itself from John Oswald’s ‘Plunderphonics’ in that the plundering occurs in a live situation and that makes the performer an accomplice in the process of appropriation (of themselves). In a way, since E-tudes appropriates several live performances simultaneously, it proposes the notion of plundering in real-time, or ‘Real-Time Plunderphonics’. It is therefore important that the event take place in a live situation, as the theatrical effect of being plundered will be evident visually in relationship to the audio. Consequently, the amount of processing of the audio signals will be visible to the audience and the more processed the performances are, the more contrasting they will look in relationship to what is heard through the speakers. In E-tudes, this premise is consciously used to create a narrative that navigates, in literary terms, between the ‘real’ (actual performance) and the ‘surreal’ (more extreme processed audio). In contrast with the acousmatic tradition (music presented through loudspeakers in a fixed medium where the sound sources are not visible), the live performance makes the process of appropriation transparent to its audience as a result of the cognitive association between audio and visuals. In an acousmatic approach, a sound that is radically processed loses its characteristics and therefore the cognitive relationship between source and result may be lost. On the other hand, if the source is exposed visually in a live performance, the audience will have more audio/visual links and one may suppose that the audio processing could be even more extreme without losing the association with the source. 
%	Furthermore, E-tudes’ approach is atypical in relationship to ‘Plunderphonics’ or other music that borrows found material (for example, by musical quotation) in that plundering is not the central purpose of the creative process, but rather a tool for creating a new idiosyncratic audio/visual result. This difference is rather important since it addresses the question inherent in the ambivalence of plundering oneself to create something new as opposed to performing something new in an immediate and direct fashion. Therefore, the idiosyncratic result justifies the conscious participation of the performer in a piece in which what he or she plays is not directly heard by the audience.  This position proposes a new relationship between performer and composer and it also presents a new approach to composition. The composer’s role is not to establish direct communication with the performer (through a score or oral tradition) but rather to use live audio signals of existing music as building blocks to create a new work. All of this is achieved by writing computer software (using SuperCollider 3 – a programming language specialized in audio applications) specifically for the piece. Moreover, E-tudes takes a didactic attitude toward the process of appropriation by giving the listener access to the processed and unprocessed building blocks to show the different layers within the composition, not with the intention of being explicit, but to engage and establish a relationship with the listener. Finally, this composition combines the use of improvisation and generative music to have an unfixed output that changes for each performance of the work. This enables the piece to run in a loop during a long extended time frame without repeating itself. Every time the piece will be played not only will the audiences’ experiences differ, because of their own choices, but also the content of the piece itself will vary.
%	E-tudes takes many elements used before in electronic music and live performance such as improvisation, appropriation, generative music, installation and traditional performance practice, and by combining them points to a development in performing with live electronics. By introducing a dynamic group of live performers and an appealing and interesting visual scenario, this event deals with the problematic of the lack of visual clues and theatrical elements that live electronics performance has faced since its beginning. Hopefully, it will also encourage other creators that deal with live electronics to think seriously about the visual, theatrical and ritualistic aspects of performance. This composition will also contribute to instigating awareness within the contemporary music community on how the presentation of a piece can be as crucial as the sound. It also proposes that the creator is able to innovate by searching for new ways that the audience relates to the work. The event will also contribute to the creative development of the artists because it will give them the opportunity to try out and experiment on the various interactive and performative aspects of the piece and later examine and evaluate how these processes may be improved.

%Didactic attitude towards appropriation: listener will access processed and unprocessed building blocks, not with the intention of being explicit, but to engage and establish a relationship with the audience.

%Relational aspect: it proposes the idea that one may innovate by searching for new ways that the audience relates to the work.

%Elements of improvisation and generative music. Every time the piece will be played not only will the audience’s experience differ, because of their own choices, but also the content of the piece itself will vary.

\section{On Violence}

This composition attempts to explore the aesthetics of violence and reflect on different manifestations of violence. It is also inspired by slovenian philosopher Slavoj Zizek's ideas about violence. Zizek  categorizes violence into two main types: subjective and objective violence. Subjective violence is clearly identifiable by an agent, for example acts of terror or crime, and it is perceived as a clear interruption of the normal state of things. On the other hand, objective violence is  violence that is inherent in the social fabric and it is hard to see and experience for the advantaged classes or countries. What Zizek argues is that objective violence is inherent within the social "balance" and it is objective violence which triggers acts of subjective violence. Furthermore, Zizek identifies two types of objective violence: symbolic and systematic violence. Systematic violence is manifested through our economic and political systems that in order to give the idea of a normal smooth running of things, exert systematic violence on large groups of people. Symbolic violence is related to and included within systematic violence but it is specific to violence expressed through language and other symbolic systems (like music). Zizek goes further to argue that the forms of symbolic violence are actually based on and manifested by the symbolic systems as such. 

\section{\v{Z}i\v{z}ek!?}

Zizek? is a computer-mediated improvisation that gives a live alternative soundtrack to the Zizek! (2005) movie. Each performer has a laptop in front of them. The laptops are connected through a network by which the composer guides the improvisers by sending them written directions, animations (moving graphical notation) and through headphones, an aural score that consists of sound and music derived from the audio of the film. 

Alexander Hawkins (piano), Dominic Lash (double bass), Javier Carmona (drums)

\section{FreuPinta}

%\subsection{Simulation Series}
%\subsection{Occupation Series}
%\subsection{Transgression Series}

\section{Improvisations}

%\subsection{Horatio Oratorio}
%\subsection{Perro-chimp}
%\subsection{Too Hot to Handel}
%\subsection{Mowgli}

\label{ch:compositions}