\hypertarget{chapter6}{}
\chapter{Compositions}

This chapter briefly describes the portfolio of submitted work. It focuses exclusively on technical and musical descriptions of the musical output and is not intended to serve as a wider commentary on the submitted work---the previous chapters already serve as a meta-commentary on the wider concerns underlying the creative process and musical output. This portfolio I believe reflects the various musical practices that I have been involved in during the research period, which include composing, improvising and performing as well as writing computer programmes. Furthermore, within my creative practice these different activities are often interconnected, resulting in work that is sometimes difficult to categorize using only these traditional definitions. Therefore, the submitted work also includes a wide variety of outcomes which may be identified as different types of work. I tried not to exclude any of the various types of musical outcome that make up my creative practice, although by doing so I am aware that my work might appear to lack focus. Nevertheless, I consider the diversity of outcomes from my practice to be a feature of my creative process and, instead of repressing this characteristic of my work, I attempt to embrace it. For that reason the type of work presented as part of this portfolio includes simple as well as more ambitious projects, some of which are ongoing and could be perceived as unfinished. As my work seeks to rework musical strategies that might be regarded as fundamental to the way in which we traditionally create, perform and experience music, my practice is necessarily experimental in nature and I consider it is important to present not only work which presents successful outcomes, but also failed and incomplete experiments. I have divided the portfolio of work in five main projects. The first project is \emph{E-tudes}, a set of four pieces for live electronics and six pianists playing keyboards, a work which can be presented as a concert performance or as an installation with performative elements. I will describe the basic set-up and computer programmes that are fundamental to the way in which all of the four pieces are performed and presented, later examining each individual `e-tude' in more detail. The second project I will examine is a piece called \emph{On Violence} for piano, live electronics, sensors and computer display. I will describe how this work is performed and how it was composed, including the way in which the multimedia score combines conventional with real-time notation and how the pianist interacts with the score, sensors and live electronics. The third project is \emph{\v{Z}i\v{z}ek!?}, a \mbox{computer-mediated-performance} for three improvisers that serves as an alternative soundtrack to a documentary about Slovenian philosopher Slavoj \v{Z}i\v{z}ek. I will explain how the improvisatory elements are incorporated within a pre-composed structure in this performance through real-time scoring strategies and how the final musical outcome is related to the audio of the documentary itself. The fourth project is a collection of short pieces called \emph{FreuPinta}. I will explain the nature of these small experiments, including the reason why I think they have aesthetic value on their own and belong with the rest of the submitted work. The experimental pieces fall into different groups based on their characteristics and I will briefly examine the reasoning behind their creation. Finally, I will briefly describe the fifth project which consists of a selection of different improvisations that I devised or participated in using the computer environment I developed for live improvisation.

\section{E-tudes}

\emph{E-tudes}\footnote{See \hyperlink{portfolio}{Contents of Portfolio}: \href{http://phd.federicoreuben.com/contents/audio/compositions/}{CD I (\emph{Compositions})}, Tracks 1--4 and \href{http://phd.federicoreuben.com/etudes/}{DVD I (\emph{E-tudes})}.} is a set of electronic \emph{\'{e}tudes} for six keyboards, live electronics and mechanical piano.\footnote{In case a mechanical piano is not available, it is possible to use a sampler with piano sounds.} These compositions were written for the ensemble \textbf{piano}\emph{circus}\footnote{See \href{http://www.pianocircus.com/}{\texttt{http://www.pianocircus.com/}}} for a project that became a \mbox{two-year} collaboration and led to two performances.\footnote{Enterprise 08 Festival, The Space, London, May, 2008, and The Sound Source, Kings Place, London, July, 2009.} What initially attracted me to this ensemble was its very particular instrumentation consisting of six electronic keyboards. I thought this would be a suitable platform to experiment with the notions of \emph{real-time plunderphonics} and \emph{live musica derivata},\footnote{See \hyperlink{realtimeplunderfuck}{pp. 91--92}.} considering that these instruments are electronic and therefore produce almost no audible acoustic sound.\footnote{The only acoustic sounds that can be heard are the keyclicks produced by the physical contact with the keyboards while playing. This noise is slightly audible mostly when there are no sounds playing through the speakers (or they are very quiet).} Like a book of \emph{\'{e}tudes} from the repertoire, \emph{E-tudes} consists of a set of pieces that can be performed together at the same event or individually as separate short pieces. At present I have completed four `e-tudes'; as it is an ongoing project, I will continue adding new pieces to the collection. \emph{E-tudes} is modular in the way in which it can be presented: depending on the set of circumstances for a given event, they can be presented separately or as a whole, as a concert performance or as an installation with performative elements. In the installation version, the audience walks into, out of, and around the area surrounding the musicians and has creative control over how they want to experience the performance. By choosing between listening to the speakers in the room or to various headphones which are distributed through the performance space and which generate different outputs, each member of the audience fabricates their own version of the piece. In the installation version there are various possible outputs generated by the computer from the performance from which the audience can choose. It is also possible to have a performance where the members of the audience are wearing wireless headphones that can receive multiple channels that are transmitted in the performance space, allowing them to choose which channel and output from the performance they want to hear.\footnote{This was the case in the performance at Kings Place.}

I use the same configuration for all of the pieces that comprise \emph{E-tudes}: the ensemble of six keyboards is placed in hexagonal formation and divided into two subgroups. The first subgroup, consisting of three pianists, are asked to select \emph{\'{e}tudes} from the western piano repertoire at will---they can select the \emph{\'{e}tudes} they prefer to perform (for example, \emph{\'{e}tudes} by Chopin, Debussy or Ligeti, to mention just a few)---and are asked to play them in their chosen order during the duration of the performance. The second subgroup, consisting of the remaining three pianists, perform together from \emph{The Sixth Book of Madrigals} by Don Carlo Gesualdo da Venosa (1566-1613). The pianists playing the madrigals send Midi information to a computer that transforms the audio signal from the \emph{\'{e}tudes} and schedules the computer-processing events. The audience is not able to hear in the room what the pianists are playing as the keyboards do not produce an acoustic sound. The seventh performer (performing the live electronics part)\footnote{In the previous performances of \emph{E-tudes}, I have performed the live electronics part myself.} performs different tasks: at some points s/he speaks the Madrigals' text into a microphone and the spectral information from this signal is used to process the final audio output and to trigger other sound events; at other times s/he mixes the resulting sounds, controls different parameters in the computer processing and triggers sounds with a Midi controller. The live electronics part is not fixed, leaving space for improvisational elements within the human/computer interaction. Finally, through the analysis of all the inputs the computer sends Midi messages to the mechanical piano, adding yet another element to the performance. In the room the final result of the creative process of combining the simultaneous performances in diverse arrangements is diffused through the speakers. In the installation version, the headphones that are spread through the performance space portray the inner life of the performance sounding in the room and reveal the inner layers of computer processing as well as the appropriated compositions.

Computer programmes play a vital role in \emph{E-tudes} and were written in SuperCollider---some of these programmes are discussed in \hyperlink{chapter5}{Chapter 5} but some were exclusively written for \emph{E-tudes}.\footnote{The code for these computer programmes can be found at \href{http://github.com/freuben/Etudes}{\texttt {http://github.com/freuben/Etudes}}.} These programmes are used to analyze incoming Midi data to schedule events\footnote{See \hyperlink{miditrig}{p. 113}.} and for digital signal processing (DSP). The incoming Midi events from the pianists playing Gesualdo are analyzed and divided into each voice of the original madrigals. The computer uses score following techniques to track each voice and according to its position in the score, schedules specific DSP events. The Midi \emph{note} and \emph{velocity} information in some occasions is used to determine certain parameters in the DSP algorithms. The DSP algorithms of the live electronics use as input two major audio sources: the input of the combined live audio of the sound generated by the three pianists playing \emph{\'{e}tudes} and \emph{micro} elements\footnote{See \hyperlink{macroplunder}{pp. 89--91}.} derived from various recordings of existing music which I chose to appropriate. The individual live audio signals coming from each pianist playing \emph{\'{e}tudes} are interpolated\footnote{What I mean by interpolation is a transformation of one sound to another through a process that does not necessarily fit the description of `morphing'.} with one another (by altering the pitch and volume of the signals).\footnote{Each signal is interpolated with the other by gradually pitch-shifting one signal down four octaves and fading out its volume gradually, while at the same time introducing the next signal which would be pitch-shifted four octaves down and gradually transposing it up until its original pitch, and by gradually fading it in.} The live electronics performer can change the duration of the interpolation between \emph{\'{e}tudes} with the Midi controller. At the same time, the resulting signal is then pitch-shifted again through several pitch ratios (the original signal results in five different signals with varying pitch) generating multiple signals which are then mixed together. The sounding result is a very noisy signal which could be described as `piano noise' (it still retains a piano-like quality). I then utilize the `piano noise' as input in synthesis algorithms which filter it using several techniques. The `piano noise' is very different to \emph{white noise}, \emph{pink noise} or any other types of noise used in classic synthesis techniques in that its spectral flux is constantly changing and its rate and amount of change is fairly irregular. Additionally, the live electronics performer can change the sonic qualities of the `piano noise'---and therefore also change its spectral flux---by altering the interpolation time of the live audio signals coming from the pianists. Some of the `e-tudes' in their final result (the output diffused through the speakers) are composed exclusively using synthesis algorithms which use this `piano noise' as input. At the same time, in the installation version, the audience can listen through headphones to the different outputs at different degrees of processing---for instance, one of the headphone outputs consists entirely of material generated from the interpolation of \emph{\'{e}tudes}, while another one reveals the `piano noise'. In certain headphone outputs the original appropriated sources (the \emph{\'{e}tudes} and the madrigals by Gesualdo) also appear closer to their original form. The algorithms that control the overall process have generative elements---each time they are performed, they generate different results. The generative characteristics of the algorithms, the varying incoming data from the live performances (the \emph{\'{e}tudes} chosen by the pianists change for each performance of \emph{E-tudes}\footnote{Like Cage's \emph{Imaginary Landscape No.4}, this type of musical appropriation is current, generative and indeterminate. See \hyperlink{landscape4}{p. 81} for a further discussion of Cage's appropriation strategy.} and the incoming Midi data from the madrigals varies each time they are performed) as well as the improvisatory elements in the live electronics performer's part, makes \mbox{\emph{E-tudes}} an electronic composition that changes (both in content and performance) each time that it is presented, while also maintaining certain elements that identify it as the same composition.

\emph{E-tude I} is based on the first madrigal of Gesualdo's \emph{Sixth Book of Madrigals} called \emph{Se la mia morte brami}. In \emph{E-tude I}, `piano noise' is filtered in different ways using various subtractive synthesis algorithms. Several algorithms take the live electronics performer's voice reading the words of the madrigal as input to filter the `piano noise' using different filtering techniques. The most prominent filtering techniques using the voice as input are vocoding (using a variation of the `classic' vocoder algorithm) and the FFTFilter described in the previous chapter.\footnote{See \hyperlink{fftfilter}{pp. 98--99}.} The microphone signal is also used for onset detection, and the live electronics performer may trigger different percussive sounds (generated by filtering bursts of `piano noise' at different frequency ranges) with his/her voice. Spectral gating (an FFT technique which ignores the frequency bins which have magnitudes below a certain threshold) of a limited frequency range of the `piano noise' is another technique used to isolate the strongest frequencies in a specified range---the resulting frequencies are used as pitched material that is presented either in its natural sinusoidal quality (prominently in the high frequencies) or these frequencies are mapped into Midi notes which trigger the mechanical piano (mostly using its lower range). At the same time, all pitched material (including the center frequency of some filtered sounds) is altered or defined by the Midi note information received from the pianists performing the madrigal (tuned in \emph{just intonation}). The dynamics for these sounds are shaped by the Midi velocity from the performance of Gesualdo. At the same time, the different layers of sound are modified such that they have a similar phrase structure to the madrigal---the layers start and end at the same point in time as the madrigal.\footnote{I use this technique in many of my compositions: I plunder the phrase structure of an existing composition to generate a blueprint for a new composition. See \hyperlink{macroplunder}{pp. 89--90}. I also wrote a computer programme that automizes this process and generates a visual representation of phrase structure using a Midi File as input. See \hyperlink{scorevisual}{pp. 109--112}.} The Midi note information (mostly note-on messages, but not exclusively) at times also trigger different sounds generated through a combination of filtered `piano noise' and data derived from analysis of plundered recordings. A common technique used in \emph{E-tudes} to make synthetic sounds sound more imperfect or `natural' sounding is to modify the synthesized sound according to the extracted fundamental of a recorded instrumental or vocal sound---the synthetic sound becomes more irregular and therefore sounds more `natural' due to the imperfection it inherits from the instrumental or vocal sound. Another technique I use to make synthesis algorithms sound more `instrumental' is by deriving harmonic structures (the fundamental and partials of a sound) from FFT analysis of recordings of the instruments I want to approximate. In \emph{E-tude I}, I generate sounds using these techniques to approximate sounds with similar characteristics to a celesta, several percussion instruments, to a vocal melody, high bowed string harmonics, etc. I also use FFT to combine spectra between the instrumental recordings and the `piano noise'. \emph{E-tude I} starts with a fairly `abstract' sound world---further removed from recognizable `traditional' instruments or music genres and including sounds that are more noise than pitch based---gradually transforming into a more `referential' sound world, made up of pitched sounds, periodic rhythms, more identifiable sounds modeled on existing instruments and pitch material that is more referential to other more recognizable musical genres. The process of transformation culminates with the emergence of a prominent melody (driven by the FFTFilter, filtering `piano noise' and following the melodic contour of a plundered recording of a vocal sound) resulting in music that is reminiscent of a song (or aria) with an \emph{ostinato} of simple and `primitive' rhythms.

\emph{E-tude II} uses \emph{Belt\`{a} poi che t'assenti} (the second madrigal in the book) as the control structure of the computer processing. In this `e-tude', the role of the `piano noise' is reduced only to a source of noise within physical modeling synthesis algorithms, which create sounds reminiscent of instrumental sounds. These sounds include the high frequency sound based on a physical model of a bowed string which ended \emph{E-tude I}, this time changing in pitch more gradually and gliding through what sounds like a variation of the main melody of the `song' with which \emph{E-tude I} culminates. The algorithms generate sounds reminiscent of plucked strings, percussion and wind instruments based on physical models of Korean instruments. The `plucked string' sounds, for instance, are generated using a variation of the Karplus-Strong plucked-string algorithm using a burst of `piano noise' as its excitation and altering it in frequency and amplitude through the extracted fundamental of a recording of a plucked \emph{Geomungo} (Korean instrument) string. In addition to the sounds generated by the synthesis algorithms, the mechanical piano produces pitched material consisting of ascending \emph{arpeggios} generated through vowel sounds that are \emph{synthrumentized}, which the live electronics performer vocalizes into the microphone.\footnote{The PartialTracker class is used to detect the strongest frequencies of these vowel sounds to generate notes for the \emph{arpeggios}, see \hyperlink{partrack}{p. 97}.} The prominent melodic sound that emerges on its own at approximately the middle of \emph{E-tude II}, is generated through a combination of `classic' filtering techniques (band pass, high pass, dynamic bank of resonators, etc.) which filter the `piano noise' to try to emulate a `wind instrument' with vocal characteristics. The attack and release envelopes of this sound are derived from a recording of a recorder and after the sound is released, a high-frequency noise reminiscent of human breathing follows (the frequency range of the noise was mapped approximately to the frequency range of a person's breathing-in sound). This `virtual instrument' plays a melody that is plundered from a recording of \emph{Gagok}---a traditional form of Korean vocal music.\footnote{See \hyperlink{rockwell}{Rockwell (1972)} for an introduction to \emph{Gagok}.} After a brief solo section, an ensemble of `virtual instruments' joins the melodic sound and emulates music reminiscent of \emph{Gagok}---including two `virtual instruments' (one of them sounds more `sinusoidal' and the other sounds closer to a reed instrument) that play the melody with slight deviations in timing and tuning. These deviations are driven by a generative algorithm (meaning that each time that \emph{E-tude II} is performed, the melodic deviations vary) that is designed to emulate the types of improvised melodic elaborations found in original \emph{Gagok}. During the duration of \emph{E-tudes II}, the rate at which sporadic events (plucked strings, percussion instruments and mechanical piano \emph{arpeggios}) take place gradually becomes faster and the events themselves become more active and unpredictable (for instance the mechanical piano \emph{arpeggios} become faster and with more notes and their direction starts changing randomly as they become more active), until they are squashed against each other and become cluttered at the end of the composition.

\emph{E-tude III} is based on the third madrigal by Gesualdo called \emph{Tu piangi, o filli mia}. This `e-tude' opposes synthesized pitched sounds with a strong fundamental frequency (some of them sound almost like sine waves) at the beginning with the `piano noise' that is revealed for the fist time in its unfiltered and unprocessed form later in the composition. The pitched sounds consist mainly of three different types of sounds: two sustained sinusoidal pitches, `celesta' sounds and a single descending \emph{arpeggio} of `plucked string' sounds. The two sinusoidal pitches slowly change in frequency, first gradually detuning away from each other and later changing direction until they slowly approach each other, producing \emph{beating} before merging into the same tone. The `celesta' sounds are generated in the same way as the ones that were used in \emph{E-tude I} (generated from filtered `piano noise') and derive their pitch material from spectral data (\emph{Synthrumentation}) of the audio signal of just one of the pianists playing \emph{\'{e}tudes}. Their rhythm is triggered through onset detection of the live microphone signal (the voice of the live electronics performer reading the text of the madrigal). The descending \emph{arpeggio} of `plucked string' sounds---which happens only once and is reminiscent of the mechanical piano \emph{arpeggios} in \emph{E-tude II}---is generated through the same Karplus-Strong algorithm used in \emph{E-tude II}, but instead of using a plundered recording of a \emph{Geomungo}, it uses a recording of a harp. The `piano noise' is first revealed as short percussive sounds (the `piano noise' is filtered so that it sounds like footsteps) triggered by the live electronics performer with the Midi controller at periodic intervals. Then it is revealed as filtered noise in the high frequencies and later in the low frequencies---the filtered noise originates from a selection of FFT bins of `piano noise'. In \emph{E-tude III}, there is a slow transition between pitched sounds and `piano noise', which gradually becomes dominant as the spectrum is gradually filled with FFT bins---culminating in the complete frequency spectrum of `piano noise' being diffused equally over the speakers. At the end, the `piano noise' vanishes completely in a matter of seconds (through the reverse FFT process) followed by silence, and a series of sporadic and aggressive bursts of clusters played by the mechanical piano and by short phrases of sinusoidal sounds and \emph{synthurmentized} piano chords revealing harmonies from the \emph{\'{e}tudes}. 

\emph{E-tude IV} derives information from three main sources: Gesualdo's forth madrigal called \emph{Resta di Darmi Noia}, the \emph{\'{e}tudes} chosen by the pianists and several appropriated recordings of Pygmy music.\footnote{\hyperlink{pygmy}{Mbuti Pygmies of Ituri Rainforest (1992)}.} Pitched material is derived from recordings of Pygmy music using the SpearToMIDI and PartialTracker programmes described in the previous chapter.\footnote{See \hyperlink{spectrack}{pp. 95--103}.} The results from the different types of analysis are then selected and combined according to the desired musical outcome and stored as Midi note data. The spectral data of the \emph{\'{e}tudes} is combined with the collected data from the Pygmy music recordings and the result is modified by the melodic and harmonic material from the madrigal. The Midi information resulting from this process is realized by the mechanical piano, which is the only source producing sound, becoming the `virtual' soloist of \emph{E-tude IV}. At some points, the melodic contour from the different voices from the Gesualdo is used to modify the tempo of the different layers of the Midi note data derived from the Pygmy music---if the shape of melody goes up, the tempo progressively becomes faster and if the melody goes down, the tempo becomes slower. In addition, certain algorithmic composition strategies are used to process the final result---for example, stochastic methods are used to control note density by gradually filtering notes \emph{in} and \emph{out}. \emph{E-tude IV} uses data of three recordings of Pygmy music, which at the same time represent three sections of the composition. The first section is based on a song for the \emph{molimo} ritual,\footnote{Ibid., Track 24.} which is celebrated on special occasions such as the death of an important member of the tribe and is meant to wake up the forest from the sleep which is allowing bad things to happen to its children.\footnote{See \hyperlink{mukenge}{Mukenge (2002)}.} The middle section is based on a recording of a musical bow played by a \emph{Mbuti} pygmy,\footnote{\hyperlink{pygmy}{Mbuti Pygmies of Ituri Rainforest (1992)}, Track 15.} which morphs into the last section, derived from a recording of hunters signaling, shouting and beating.\footnote{Ibid., Track 5.}

\emph{E-tudes} is a set of compositions that explores a type of live electronics performance which attempts to establish new relationships between performer, composer and audience. It establishes at different times both \emph{interactive} and \emph{interpassive} relationships between the performer and the technological objects as well as between the musicians and the audience.\footnote{See \hyperlink{newrelationships}{pp. 46--52}.} It also seeks to establish new forms of exchange between composer and performers as well as between the performers within an ensemble.\footnote{See \hyperlink{ensembledy}{p. 53}.} \emph{E-tudes} combines the use of live electronics, improvisation, real-time computation and generative music to create a result which is unfixed, responsive and which changes for each performance of the work.\footnote{See \hyperlink{techcomp}{pp. 53--56}.} Additionally, \emph{E-tudes} attempts to find new and innovative ways to approach the process of appropriation using idiosyncratic musical strategies. First, it plunders live performances of existing music, using their audio and Midi signals as building blocks for multiple musical results, exploring the notions of \emph{real-time plunderphonics} and \emph{live musica derivata}.\footnote{See \hyperlink{realtimeplunderfuck}{pp. 91--92}.} It also appropriates \emph{micro} and \emph{macro} elements of notated material, recordings and live signals as well as treating the musical sources at varying degrees of processing, affecting our ability to recognize the original source. Finally, the sources from which \emph{E-tudes} appropriates are, unlike classic \emph{plunderphonics}, indeterminate, less familiar, obscure or exotic: music from places far removed from western culture (Korean traditional vocal music, Pygmy Music), early music (Gesualdo madrigals), pop music far from the mainstream of consumer culture and unspecified \emph{\'{e}tudes} from the repertoire.\footnote{See \hyperlink{appropstrat}{pp. 88--91}.}

\section{On Violence}
 
\emph{On Violence}\footnote{See \hyperlink{portfolio}{Contents of Portfolio}: \href{http://phd.federicoreuben.com/contents/audio/compositions/}{CD I (\emph{Compositions})}, Track 5 and \href{http://phd.federicoreuben.com/onviolence/}{DVD II (\emph{On Violence})}.} is a composition for piano, live electronics, sensors and computer display.\footnote{The code of the programmes that run \emph{On Violence} can be found at \href{http://github.com/freuben/OnViolence}{\texttt {http://github.com/freuben/OnViolence}}.} The pianist reads from a score displayed on the laptop screen, which combines conventional notation with real-time scoring.\footnote{See \hyperlink{realtimescore}{pp. 103--104} for a discussion on real-time scoring strategies.} The performer also wears headphones to receive audio triggers and cues that constitute the aural element of the score. I implemented this score using SuperCollider and the AlgorithmicScore class.\footnote{See \hyperlink{algoscore}{pp. 104--109}.} The pianist interacts with the score through two midi pedals that are used to `turn pages', display graphic notation, give written directions to the performer and activate score animations. During the duration of the performance, the score gradually alters from conventional notation to more experimental notations.\footnote{See \href{http://phd.federicoreuben.com/onviolence/score-demo/}{DVD II (\emph{On Violence}) \tiny \textgreater \footnotesize \hspace{0pt} Documentation \tiny \textgreater \footnotesize \hspace{0pt} Score Demo} for a demo of how the score might look/sound during performance.} The real-time scoring elements of \emph{On Violence} use a combination of chance, generative and spectral methods to generate visual and aural material that changes and adapts for each performance. The pianist therefore is asked to follow a score that has both fixed and unfixed indications, some of them involving spontaneous reaction and improvisation. The score information is generated in real-time by the computer and is very difficult, if not impossible, to perform accurately since the pianist does not know the exact content of the music s/he will be performing. However, the pianist is still asked to perform the score to the best of his/her ability. This type of performance strategy is deliberate and attempts to establish an \emph{interpassive} relationship between the performer and the laptop displaying the score---the pianist becomes frantically active by the `impossible' demands from the technological object which remains passive.\footnote{See \hyperlink{zizekinterpassiv}{pp. 48--49}.}

\emph{On Violence} appropriates existing music form various sources: Dieterich Buxtehude's \emph{Praeludium in G Minor, BuxWV 132}, Einst\"{u}rzende Neubauten's \emph{Autobahn} and Richard Wagner's \emph{Parsifal} and \emph{Tristan und Isolde}. It also plunders sounds including political speeches, screams, guitar feedback and metal banging. Buxtehude's \emph{Praeludium} serves as a blueprint for the composition---the other derived material is placed within its form, and is treated and processed through its shapes and contours as well as its melodic, harmonic and contrapuntal content. The live electronics part is triggered by the audio analysis of the music performed by the pianist (a microphone is placed close to the piano for real-time analysis) and by the Midi pedals. Through a combination of \mbox{machine-listening} technology and by tracking the incoming data from the Midi pedals the computer is able to follow the score, triggering different sounds and types of processing according to the structure of the composition. There are various predominant sounds in the live electronics part at the beginning of the composition: metallic bangs, vocal sounds (screams and fragments of political speeches, some of which are heavily processed), high frequency distorted sounds and a sound reminiscent to the noise produced by a motor. The metal bangs are triggered through the onset detection of the piano signal and are stretched or shortened in real-time depending on the speed at which the pianist plays. The samples of vocal sounds and the high frequency distorted sounds are selected randomly and triggered at specific moments following the pianist's playing and are also stretched or shortened in real-time depending on the tracked tempo. Some of the vocal sounds are processed by two different types of synthesis algorithms: one of them convolves two different types of weakly nonlinear oscillators which use the sample as its external force and the other convolves one weakly nonlinear oscillator with the result of a linear predictive coding (LPC) error.\footnote{The UGens used in these synthesis definitions are part of the SuperCollider SLUGens library by Nick Collins.} The high frequency sounds are spectrally gated samples of guitar feedback that are passed through a distortion guitar pedal. The sound reminiscent of a motor is turned on and off through the Midi pedals and consists of a low frequency pulse wave that is distorted through an overdrive guitar pedal. The pianist controls the frequency of the pulse wave with two 3G Force sensors (accelerometers) attached to his/her hands, which track arm movement (as the pianist lifts his/her arms, the frequency increases and as s/he lowers them, the frequency decreases). During the first section of the composition, the pianist alternates between incessantly banging chords at a periodic rhythm and controlling the `motor' sound by lifting and lowering his/her arms. The chords that s/he plays are derived from the spectral analysis (using the PartialTracker class) of Neubauten's song \emph{Autobahn} and modified both in harmonic material and register by the Buxtehude. The pitched material of the vocal and distorted high frequency sounds as well as the `motor' sound are also modified in pitch through the Buxtehude score. As the pianist's score starts to incorporate more real-time scoring elements the content of the live electronics part also gradually starts to transform. Samples of plundered recordings of Wagner's \emph{Parsifal} and \emph{Tristan und Isolde} start to emerge, however modified in their playing rate by the mapped shapes of Buxtehude's counterpoint. The Wagner samples become more prominent as the pianist is asked to improvise and react immediately to the algorithmic score, culminating in an electronics solo that is produced by a synthesis algorithm emulating an organ sound and playing notes derived from a fragment of a recording of Buxtahude that corresponds to that precise section in the blueprint. After the electronics solo the performer responds by playing the next phrase of the original Buxtehude on the piano. The composition ends with the following section, where the pianist is asked to improvise freely together with an electronic part that consists of a re-synthesized version of the prelude to the third act of \emph{Parsifal}. The synthesis algorithm used in the electronics in this section uses a dynamic bank of resonators filtering noise\footnote{I created this noise using the same algorithm I used to generate `piano noise' in \emph{E-tudes}, however controlling its amplitude with low frequency noise, to emulate bowing.} and modified in frequency and amplitude by the extracted fundamental of the instruments in a plundered recording of the Wagner, resulting in a sound that approximates a bowed string instrument. This synthesis algorithm is then used to emulate a string orchestra by programming it to perform the parts of the different instruments of the string orchestra (violins, violas, cellos, double basses) and by multiplying its results by the amount of instruments per section and adding a slight random deviation in pitch and timing for each result to imitate the sound of various instruments playing in unison. The resulting version of the Wagner prelude however is altered in pitch content to match the previous section and for that reason the notes of the original are modified through the Buxtehude harmonic material.
 
This composition is inspired by Slavoj \v{Z}i\v{z}ek's book \emph{Violence},\footnote{See \hyperlink{zizekviolence}{\v{Z}i\v{z}ek (2008)}.} in which he categorizes violence into two main types: subjective and objective violence. Subjective violence is clearly identifiable by an agent, for example acts of terror or crime, and it is perceived as a clear interruption of the normal state of things. On the other hand, objective violence is violence that is inherent in the social fabric and is hard to see and experience for the advantaged classes or countries. What \v{Z}i\v{z}ek argues is that objective violence is inherent within social ``balance'' and it is objective violence which triggers acts of subjective violence. Furthermore, \v{Z}i\v{z}ek identifies two types of objective violence: symbolic and systematic violence. Systematic violence is manifested through our economic and political systems which, in order to give the idea of a normal smooth running of things, exert systematic violence on large groups of people. Symbolic violence is related to and included within systematic violence but is specifically expressed through language (and other symbolic systems like music). \v{Z}i\v{z}ek goes further to argue that the forms of symbolic violence are actually based on and manifested by the symbolic systems themselves.\footnote{Ibid. pp. 8--63.} \emph{On Violence} attempts to explore the aesthetics of violence and reflect on the different manifestations of violence categorized by \v{Z}i\v{z}ek. It does so, not only through the type of sounds and plundered music it uses (which are suggestive of different types of violence) and the way they are processed, but also through the way in which the performance itself is set up. The pianist is asked to be violent by the sheer force and agressiveness s/he has to exert over the instrument and further violence is exerted upon him/her by the amount of information that is thrown at him/her by technology and by the pressure of having to perform difficult (if not impossible) tasks. Even though at the beginning of the composition, it seems that the pianist is the one exerting subjective violence onto the piano and controlling technology---by triggering sounds through pedals and controlling the `motor' sound through the sensors---we later realize that technology is controlling the pianist by flooding him/her with impossible demands. At the end of the composition, while the pianist is apparently free (s/he is asked to improvise freely and play what ever s/he wants) we get the impression that s/he is not the one in control and actually is the victim of objective violence imposed systematically upon him/her, not only by the system of performance and technology but also by the symbolic violence implicit in the music itself.

\section{\v{Z}i\v{z}ek!?}

\emph{\v{Z}i\v{z}ek!?}\footnote{See \hyperlink{portfolio}{Contents of Portfolio}: \href{http://phd.federicoreuben.com/contents/audio/compositions/}{CD I (\emph{Compositions})}, Track 6 and \href{http://phd.federicoreuben.com/zizek/}{DVD III (\emph{\v{Z}i\v{z}ek!?})}.} was commissioned for an event about the Slovenian philosopher Slavoj \v{Z}i\v{z}ek, which took place at The Sound Source, Kings Place, London, July, 2009. I was commissioned to write a live alternative soundtrack to Astra Taylor's \emph{\v{Z}i\v{z}ek!} (2005) documentary, which resulted in a \mbox{computer-mediated-performance} for three improvisers performing on piano, double bass and drums.\footnote{It was premiered by Alexander Hawkins (piano), Dominic Lash (double bass) and Javier Carmona (drums).} In \emph{\v{Z}i\v{z}ek!?}, each improviser has a laptop in front of them, connected through a computer network, by which the improvisers receive individual written directions, timing instructions, score animations (moving graphical notation) and through headphones an aural score that consists of music they have to interact with.\footnote{See \href{http://phd.federicoreuben.com/zizek/documentation/zizek-materials/zizek-parts/}{DVD III (\emph{\v{Z}i\v{z}ek!?}) \tiny \textgreater \footnotesize \hspace{0pt} Documentation \tiny \textgreater \footnotesize \hspace{0pt} Performance Materials \tiny \textgreater \footnotesize \hspace{0pt} Score Parts}.} The result is a real-time multimedia score that is synchronized with the film and is driven by computer programmes written in SuperCollider,\footnote{The code of the programmes that run \emph{\v{Z}i\v{z}ek!?} can be found at \href{http://github.com/freuben/Zizek}{\texttt {http://github.com/freuben/Zizek}}.} including the AlgorithmicScore class discussed previously.\footnote{See \hyperlink{algoscore}{pp. 104--109}.} The improvisers receive seven different written directions through the laptop displays, telling them how they should react during the performance:
\begin{enumerate}
\item \emph{Silence}: Don't play (or stop playing).
\item \emph{Like}: Imitate the music coming from the headphones.
\item \emph{With}: Play together with the music coming from the headphones.
\item \emph{Free}: Free to improvise at will.
\item \emph{Solo}: Improvise a solo.
\item \emph{Unlike}: Play in opposition to the music coming from the headphones. 
\item \emph{Without}: Improvise ignoring the sound from the headphones.
\end{enumerate}
The letters of the written directions are also color coded using the three colors of a traffic light: the \emph{Silence} direction is always in \emph{red} meaning the improvisers should stop playing at that point and the other directions are displayed in \emph{green} when they should play and \emph{yellow} when the \emph{Silence} direction is about to come. The performers also receive a written indication of which scene of the film is playing at a specific moment in time. The letters of these indications are in \emph{blue} while a specific scene is playing but turn \emph{pink} just before the next scene is about to start. In addition to written directions and indications, the multimedia score also includes score animations, which consist of moving graphical scores that convey some type of activity or gesture.\footnote{See \hyperlink{algoanimation}{pp. 107--108}.} Each score animation is deliberately devised for a specific instrument and is open to interpretation by the performer.\footnote{See \href{http://phd.federicoreuben.com/zizek/documentation/score-animations/}{DVD III (\emph{\v{Z}i\v{z}ek!?}) \tiny \textgreater \footnotesize \hspace{0pt} Documentation \tiny \textgreater \footnotesize \hspace{0pt} Score Animations} for the interpretation of the animations by the improvisers during the performance.} The written directions, audio score and score animations are triggered through a control structure derived from a Midi file of Johannes Ockeghem's \emph{Missa Mi-mi} (using computer-aided-composition tools described in the previous chapter).\footnote{See \hyperlink{compueraided}{pp. 109--113}.} The music of the aural score, to which the improvisers have to react, is based on different types of analysis and processing of the audio of the film; it is not only synchronized to the film's audio, but it is derived from it. The music within the aural score is the result of different types of analysis from the speech, music and other sounds that comprise the film's soundtrack. Pitch and rhythmic material as well as frequency content and dynamics are derived from the analysis of the audio signal of the soundtrack using different techniques, including tempo tracking, onset detection and spectral analysis (PartialTracker and SpearToMidi classes)\footnote{See \hyperlink{spectrack}{pp. 95--103}.} and are combined/modified through the information obtained from the Ockeghem Midi file. This process generates data structures that control synthesis algorithms or triggers sampled sounds to create the final aural score. The result is an audio score that, while retaining some characteristics of the speech and other sounds from the film, is also allusive to other styles of music as a consequence of the chosen sounds and analysis parameters.\footnote{See \href{http://phd.federicoreuben.com/zizek/documentation/score-demo/}{DVD III (\emph{\v{Z}i\v{z}ek!?}) \tiny \textgreater \footnotesize \hspace{0pt} Documentation \tiny \textgreater \footnotesize \hspace{0pt} Score Demo}.} The way in which the improvisers react to the audio score also produces an audible musical result that is related to the characteristics of speech and other sounds in the soundtrack and creates an interaction between the musicians and the sound of the film, which they might carry into the free improvisations.

\emph{\v{Z}i\v{z}ek!?} attempts to find new ways of collaborating with musicians with a background in improvisation by using computer-mediated-performance strategies that take advantage of their strengths as musicians but at the same time working within a pre-composed structure. It does so by reshaping the traditional relationships established between composer and performer through technology by using a multimedia score to transfer musical ideas and intention as well as facilitating certain types of group playing and synchronicity that otherwise would be difficult to achieve.\footnote{See \hyperlink{superscore}{pp. 39--42} for a discussion about the possibilities of using technology to produce a multimedia score which facilitates communication with and between musicians from different backgrounds.} In addition, by giving the improvisers visual directions and aural stimulus, it is possible to direct them towards a certain type of musical behavior and sound world without limiting their creative input into the performance. Furthermore, \emph{\v{Z}i\v{z}ek!?} is devised in such a way that, within a fixed structure, certain details may vary through improvisation, but within the constraints given by the visual and aural score. This computer-mediated-performance shares certain characteristics with generative music: the composer does not specify every detail of the result of the performance but creates the possibility for an infinite number of outcomes which share similar characteristics.\footnote{See \hyperlink{realtimepos}{pp. 54--56} for a further discussion about the relationship between improvisation and real-time computation, generative music and interactive systems.} \emph{\v{Z}i\v{z}ek!?} also explores the idea of establishing \emph{interpassive} relationships between the performers on stage and the technological objects (the laptops in front of them).\footnote{See \hyperlink{interpassiv}{pp. 46--50}.} Like \emph{On Violence}, this performance is devised in such a way that the technological object remains passive (the laptops do not produce sound or activity that is apparent to the audience) while requiring the performers to remain (hyper)active. The improvisers delegate their passivity to the technological object, giving the semblance of reality to the illusion that they are in control, when they are actually following the demands from the laptops. Finally, \emph{\v{Z}i\v{z}ek!?} deals with the process of appropriation slightly differently from \emph{plunderphonics} or \emph{musica derivata}. The composer creates a new musical result by appropriating live performances as opposed to creating music through plundered recordings or by writing a score based on derived material from existing music to be realized by either classically trained musicians or mechanical instruments. This is achieved by appropriating the performance of musicians with a background in improvisation and by manipulating the output of their improvisation through the multimedia score. \emph{\v{Z}i\v{z}ek!?} appropriates live performances differently from \emph{E-tudes}; instead of plundering different types of signals from live performances to create a new composition the process of appropriation starts by choosing living improvisers (rather than recordings, audio and Midi signals of existing music), manipulating/directing their creative output towards a desired musical result. Furthermore, \emph{\v{Z}i\v{z}ek!?} appropriates a film into the performance and superimposes live music on top of its original soundtrack so that the live music performance interferes with the appreciation of film by trying to steal the audience's attention from it. The activity and volume of the improvisation (the music is meant to be louder than conventional film music, sometimes masking the audio of the film) at some points competes with \v{Z}i\v{z}ek's own relentlessness as a speaker. At the same time, however, as a consequence of the relationship between the live music and the audio of the soundtrack, the performance amplifies \v{Z}i\v{z}ek's own hyperactivity, suggesting that he is not the analyst but the analysand. The result of the performance as a whole is an overload of information for the audience who, not being able to grasp the whole content of the performance (the meaning of the concepts being discussed, the images of the documentary and performance, as well as the detail in the music) has to give something up---the audience might (consciously or unconsciously) decide to ignore the music in order to try to concentrate on understanding what is being said or might choose to perceive the performance as sound, rendering \v{Z}i\v{z}ek's speech as music and detracting from its meaning as language. The audience may also have to come to terms with the impossibility of fully grasping what is presented at them, stop trying to understand all of the different elements of the performance and perhaps give up on it entirely, becoming passive spectators of an `empty ritual'.

\section{FreuPinta}

\emph{FreuPinta}\footnote{See \hyperlink{portfolio}{Contents of Portfolio}: \href{http://phd.federicoreuben.com/contents/audio/freupinta/}{CD II (\emph{FreuPinta})}.} is a collection of small experimental pieces that were realized in parallel to the more ambitious musical projects. They are often small experiments, studies, fragments and residues of the bigger pieces. By themselves they might not be considered as significant achievements but, nevertheless, I consider they still have some aesthetic value on their own. During the research period, I produced several of these short musical experiments in parallel to some the main computer programmes I developed, often as a testing bench of their functionality.\footnote{See \hyperlink{chapter5}{Chapter 5} for a short description of these computer programmes.} The aesthetic value of these pieces lies, I believe, in that they were developed as consequence of an experimental process that I think reflects my broader aesthetic concerns. Many of these short experiments are concerned with finding new interesting approaches to musical appropriation.\footnote{See \hyperlink{chapter4}{Chapter 4} for a discussion of musical strategies that deal with appropriation as a creative tool.} The way in which they deal with the process of appropriation is often more `straightforward' than the rest of the submitted work and reveals more directly and apparently the appropriation of only one piece of existing music. Many of these pieces are the result of documented experiments in the development of computer programmes that aimed at finding new types of relationships between performer, composer and audience through technology.\footnote{See \hyperlink{chapter3}{Chapter 3} for a further discussion on how technology might help reshape relationships between people involved in making and experiencing music.} A considerable number of pieces from this collection were also the result of experiments in generative, spectral and other algorithmic techniques that were implemented using real-time computation and interactive systems. \emph{FreuPinta} was also the platform where I first experimented with synthesis algorithms and physical models before implementing them in the bigger projects. My first experiments with \emph{real-time plunderphonics} and \emph{live musica derivata} are also documented as part of this collection of short pieces.

Some of the pieces that comprise the \emph{FreuPinta} collection are divided into different series of works. The \emph{simulation series} is comprised of four short pieces that deal with appropriation by attempting to simulate human performances of existing music through technology. `Artificial' performances are therefore created by appropriating recordings of human musicians playing different types of music and attempting to emulate these performances using tools provided by digital technology. The process of simulating human performance however is not with the purpose of reproducing the original music but of transcribing the music to be realized by `virtual' performers. The `artificial' performance is realized by analyzing the recording of the original performance using SpearToMidi\footnote{See \hyperlink{spearmidi}{pp. 101--103}.} (using different analysis parameters) to generate a `virtual' score that may be performed either by synthesis algorithms that emulate acoustic instruments or by mechanical instruments. For instance, \mbox{\emph{Simulation No. 1}} is a `virtual' performance of the overture to Handel's \emph{Solomon} oratorio realized by the computer using different `intrumental' sounds, therefore transcribing and `re-orchestrating' the original version. The process of transcription from the version performed by humans to the computer performance also introduces new sounds, pitches and rhythms that are idiosyncratic to the computer medium. \mbox{\emph{Simulation No. 2}} is a simulation of Radiohead's \emph{Pyramid Song} which derives data from the original recording that is performed by a mechanical piano. \emph{Simulation No. 3} and \mbox{\emph{Simulation No. 4}} present material that did not make it to the final version of the electronic part of \emph{On Violence}. They are simulations that use synthesis algorithms (which emulate an organ and a string orchestra respectively) that perform music derived from the computer's analysis of short fragments of recordings of Wagner (Prelude to third act of \emph{Parsifal}) and Buxtehude (\emph{Praeludium in G Minor}). The \emph{occupation series} comprises of three short pieces that deal with appropriation in a similar way: they appropriate a recording which is not processed or transformed at all and is left in its original form, however adding new sounds on top of the recording that are mixed with the original.\footnote{This series therefore uses a strategy similar to Duchamp's notion of \emph{assisted readymades}. See \hyperlink{lhooq}{pp. 65--66}.} The added sounds at the same time are also derived from the analysis of the recordings and are documented `performances' that use real-time computation---I used PartialTracker\footnote{See \hyperlink{partrack}{p. 97}.} and onset detection algorithms with different varying parameters that I had control over during the performance to trigger the new sounds. These sounds were recorded and mixed with the original, recontextualizing the meaning of the appropriated musical source. \emph{Occupation No.1} is a small personal homage to Stockhausen's \emph{Hymnen}---it appropriates the recording of the national anthem of Costa Rica (which is missing from Stockhausen's piece) and adds \emph{synthrumentized} piano chords on top of it. \emph{Occupation No.2} is a study for \emph{\v{Z}i\v{z}ek!?} which adds instrumental sounds (sampled guitar and percussion instruments) and sine waves to the recording of a lecture by \v{Z}i\v{z}ek---the instrumental sounds at the same time are derived from \v{Z}i\v{z}ek's speech. \emph{Occupation No.3} appropriates a fragment of Giovanni Palestrina's \href{http://en.wikipedia.org/wiki/Missa_Papae_Marcelli}{\emph{Missa Papae Marcelli}} and adds a `virtual' jazz trio that `swings' along with the original. The \emph{transgression series} uses a different approach to the process of appropriation: it plunders data from the original source and then rearranges and reshuffles its components, re-imagining the original composition by transgressing its symbolic space. \emph{Transgression No. 1} re-imagines the first movement of Mozart's \emph{Sonata in C KV 545, I}, while \emph{Transgression No. 2} envisions an alternative version of the third movement of Bartok's \emph{Suite, Op.14, Sz62}. The remaining pieces in the \emph{FreuPinta} collection include more experiments controlling synthesis algorithms with data derived from spectral analysis of existing music (\emph{Chorus I}, \emph{Chorus II} and \emph{Artaud Dei}), my first experiments with the notions of \emph{real-time plunderphonics} and \emph{live musica derivata} (\emph{Canzione Piramide}), experiments with interactive systems for live improvisation (\emph{Pianpeta}, \emph{Robojazz} and \emph{Sporcizia Vocale}) and mapping data from language and robotic movement to music (\emph{Danzonne} and \emph{Walking Head}).

\section{Improvisations}

The improvisations presented as part of the submitted work\footnote{See \hyperlink{portfolio}{Contents of Portfolio}: \href{http://phd.federicoreuben.com/contents/audio/improvisations/}{CD III (\emph{Improvisations})} and \href{http://phd.federicoreuben.com/improvisations/}{DVD IV (\emph{Improvisations})}.} are the outcome of the work I have done in collaboration with other improvisers using the computer environment I developed in SuperCollider for live improvisation.\footnote{See \hyperlink{improvprog}{p. 113}.} The recordings of these improvisations are the result of several concerts and rehearsals that took place during the period of research. The submitted work includes recordings of several performances:
\begin{enumerate} 
\item \emph{Live@ICA}: concert of improvised music for a live performance art event called \emph{The Scuttler} presented by boyleANDshaw, which took place in June, 2010 at the Institute for Contemporary Art (ICA) in London. Free improvisation for live electronics, percussion, processed voice and guitar.\footnote{The performance featured Javier Carmona (percussion), Adam de la Cour (voice/guitar) and Federico Reuben (live electronics).}
\item \emph{Too Hot to Handel}: I was asked to present a work at the Handel House Museum. The museum is located on the upper floors of 25 and 23 Brook Street in London. 25 Brook Street was the residence of George Frideric Handel between 1723 and 1759 and 23 Brook Street was the home of Jimi Hendrix from 1968 to 1969. I decided to present a site-specific event that combined the music of both Handel and Hendrix simultaneously within a structured improvisation for live electronics, voice, guitar, harpsichord, drums, double bass and narrator/conductor.\footnote{The performance featured Adam de la Cour (voice/guitar), Alexander Hawkins (harpsichord), Javier Carmona (drums), Dominic Lash (double bass), Steve Potter (voice/conductor) and Federico Reuben (live electronics).}
\item \emph{Horatio Oratorio}: \emph{Horatio Oratorio} is a performance and sound installation that employs archival sound sources, including some of the first recorded utterances and music. The music was composed and improvised by Aleksander Kolkowski and myself. The improvisations feature Stroh violin (Kolkowski) and live electronics (Reuben).\footnote{See \hyperlink{reuben}{Reuben and Kolkowski (2008)}.}
\item \emph{Live@Javier's}: Free improvisation for live electronics, drums and saxophone.\footnote{Featuring Javier Carmona (drums), Paulina Owczarek (saxophone) and Federico Reuben (live electronics).}
\item \emph{Mowgli@Cafe Oto}: Concert at Cafe Oto, Dalston, London. Structured improvisation for live electronics, voice/guitar, drums, double bass.\footnote{The performance featured Adam de la Cour (voice/guitar), Javier Carmona (drums), Dominic Lash (double bass) and Federico Reuben (live electronics).}
\item \emph{Mowgli@Modern Art Oxford}: Concert at Modern Art Oxford as part of the event \emph{Berlin Scratch Night: We Beautiful Monsters}. Structured improvisation for live electronics, voice/accordion, drums and double bass.\footnote{Featuring Adam de la Cour (voice/accordion), Fiona Bevan (voice), Javier Carmona (drums), Dominic Lash (double bass) and Federico Reuben (live electronics).}
\end{enumerate}
Only a selection of fragments from these performances are included in the portfolio. These performances are often longer but I decided not to include recordings of the complete performances - instead I have selected fragments that I think are representative of the type of work I have done in collaboration with other improvisers and show my approach to live electronics within an improvisatory context.

\label{ch:compositions}