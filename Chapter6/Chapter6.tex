\hypertarget{chapter7}{}
\chapter{Compositions}

This chapter aims to briefly describe the portfolio of submitted work. However, this chapter focuses exclusively on technical and musical descriptions of the musical output and does not have as its purpose to be a commentary on the background, motivation or theoretical framework of the creative process (the previous chapters already serve as a meta-commentary of the submitted work). I will therefore concentrate on . . . .

%During the research period, I have engaged in work that is varied in its outcome: it includes results that maybe are difficult to categorize, but which involve a varying amount of pre-compositional work. Some of my creative output therefore may be categorized as compositions, improvisations, performances, studies or spontaneous experiments.

\section{E-tudes}

\emph{E-tudes}\footnote{See \hyperlink{portfolio}{Contents of Portfolio}: CD I (\emph{Compositions}), Tracks 1--4 and DVD I (\emph{E-tudes}).} is a set of electronic \emph{\'{e}tudes} for six stage pianos, live electronics and mechanical piano.\footnote{In case a mechanical piano is not available, it is possible to use a sampler with piano sounds.} These compositions were written for the ensemble \textbf{piano}\emph{circus}\footnote{See \href{http://www.pianocircus.com/}{\texttt{http://www.pianocircus.com/}}} for a project that became a \mbox{two-year} collaboration and lead to two performances.\footnote{Enterprise 08 Festival, The Space, London, May, 2008, and The Sound Source, Kings Place, London, July, 2009.} What initially attracted me to this ensemble was its very particular instrumentation consisting of six electronic stage pianos. I thought this would be a suitable platform to experiment with the notions of \emph{real-time plunderphonics} and \emph{live musica derivata},\footnote{See \hyperlink{realtimeplunderfuck}{pp. 91--92}.} considering that these instruments are electronic and therefore produce no considerable audible acoustic sound.\footnote{The only acoustic sounds that can be heard are the keyclicks produced by the physical contact with the stage pianos while playing. This noise is slightly audible mostly when there are no sounds playing through the speakers (or they are very quiet).} Like a book of \emph{\'{e}tudes} from the repertoire, \emph{E-tudes} consists of a set of pieces that can be performed together at the same event or individually as separate short pieces. At present time, I have completed four `e-tudes', and as an ongoing project, I will continue adding new pieces to the collection. \emph{E-tudes} is modular in the way in which it can be presented: depending on the set of circumstances for a given event, they can be presented separately or as a whole, either as a concert performance or as an installation with perforative elements. In the installation version, the audience walks into, out of, and around the area surrounding the musicians and has creative control over how they want to experience the performance. By choosing between listening to the speakers in the room or to various headphones that are distributed through the performance space and generate different outputs, each member of the audience fabricates their own version of the piece. Therefore, in the installation version their are various possible outputs generated by the computer from the performance, which the audience can choose from. It is also possible to have a performance were the members of the audience are wearing wireless headphones that can receive multiple channels that are transmitted in the performance space, therefore allowing them to choose which channel and output from the performance they want to listen to.\footnote{This was the case in the performance at Kings Place.}

I use the same configuration for all of the pieces that comprise \emph{E-tudes}: the ensemble of six stage pianos is placed in hexagonal formation and divided into two subgroups. The first subgroup consisting of three pianists are asked to select \emph{\'{e}tudes} from the western piano repertoire at will---they can select the \emph{\'{e}tudes} they prefer to perform (for example, \emph{\'{e}tudes} by Chopin, Debussy or Ligeti, to mention just a few)---and are to play them in their chosen order during the duration of the performance. The second subgroup consisting of the remaining three pianists perform together from \emph{The Sixth Book of Madrigals} by Don Carlo Gesualdo da Venosa (1566-1613). The pianists playing the madrigals send Midi information to a computer that transforms the audio signal from the \emph{\'{e}tudes} and schedules the computer processing events. The audience is not be able to hear in the room what the pianists are playing as the stage pianos do not produce an acoustic sound. The seventh performer (performing the live electronics part)\footnote{In the performances of \emph{E-tudes} I performed this part myself.} performs different tasks: at some points s/he speaks the Madrigals' text into a microphone and the spectral information from this signal is used to process the final audio output and to trigger other sound events, and at other times s/he mixes the resulting sounds, controls different parameters in the processing and  triggers sounds with a Midi controller. The live electronics part is not fixed, leaving space for improvisational elements within the human/computer interaction. Finally, through the analysis of all the inputs the computer sends Midi messages to the mechanical piano, adding yet another element to the performance. In the room the final result of the creative process of combining the simultaneous performances in diverse arrangements is diffused through the speakers. In the installation version, the headphones that are spread through the performance space portray the inner life of the performance sounding in the room and reveal the inner layers of computer processing and the appropriated compositions.

Computer programmes play a vital role in all the elements of \emph{E-tudes} and were written in SuperCollider---some of these programmes are discussed in \hyperlink{chapter5}{Chapter 5} but some were exclusively written for \emph{E-tudes}.\footnote{The code for these computer programmes can be found at \href{http://github.com/freuben/Etudes}{\texttt {http://github.com/freuben/Etudes}}.} These programmes are used to analyze incoming Midi data to schedule events\footnote{See \hyperlink{miditrig}{p. 113}.} and for digital signal processing (DSP). The incoming Midi events from the pianists playing Gesualdo is analyzed and divided by each voice of the original madrigals. The computer analysis using score following techniques tracks each voice and according to its position in the score, schedules specific DSP events. The Midi \emph{note} and \emph{velocity} information in some occasions is used to determine certain parameters in the DSP algorithms. The DSP algorithms of the live electronics use as input two mayor audio sources: the input of the combined live audio of the sound generated by the three pianists playing \emph{\'{e}tudes} and \emph{micro} elements\footnote{See \hyperlink{macroplunder}{pp. 89--90}.} derived from various recordings of existing music which I choose to appropriate. The individual live audio signals coming from each pianist playing \emph{\'{e}tudes} are interpolated with one another (by altering the pitch and volume of the signals).\footnote{Each signal is interpolated with the other by gradually pitch-shifting one signal down four octaves and fading out its volume gradually, while at the same time introducing the next signal which would be pitch-shifted four octaves down and gradually transposing it up until its normal pitch, and by gradually fading it in.} The live electronics performer can change the duration of the interpolation between \emph{\'{e}tudes} with the Midi controller. At the same time, the resulting signal is then pitch-shifted again through several pitch ratios (the original signal results in five different signals with varying pitch) generating multiple signals that are then mixed together to create yet another signal. The sounding result of this last signal is a very noisy signal which could be described as `piano noise' (it still retains a piano-like quality). I then utilize the `piano noise' as input in synthesis algorithms which filter it using several techniques. The `piano noise' however is very different to \emph{white noise}, \emph{pink noise} or any other types of noise used in classic synthesis techniques in that its spectral flux is constantly changing and its rate and amount of change is fairly irregular. Additionally, the live electronics performer can change the sonic qualities of the `piano noise'---and therefore also change its spectral flux---by altering the interpolation time of the live signals coming from the pianists playing \emph{\'{e}tudes}. Some of the `e-tudes' in their final result (the final output diffused through the speakers) are composed exclusively using synthesis algorithms which use this `piano noise' as input. At the same time, in the installation version, the audience can listen through headphones to the different outputs at different degrees of processing---for instance, one of the headphone outputs is made out entirely of material generated from the interpolation of \emph{\'{e}tudes}, while another one reveals the `piano noise'. The original appropriated sources (the \emph{\'{e}tudes} and the madrigals by Gesualdo) are also displayed closer to their original form through certain headphone outputs. The algorithms that control the overall process have also generative elements---each time they are performed, they generate different results. The generative characteristics of the algorithms, the varying incoming data from the live performances (the \emph{\'{e}tudes} chosen by the pianists change for each performance of \emph{E-tudes}\footnote{Like Cage's \emph{Imaginary Landscape No.4}, this type of musical appropriation is current, generative and indeterminate. See \hyperlink{landscape4}{p. 81.}, for a further discussion of Cage's appropriation strategy.} and the incoming Midi data from the madrigals varies each time they are performed) as well as the improvisatory elements in the live electronics performer's part, makes \emph{E-tudes} an electronic composition that changes (both in content and performance) each time that it is performed, however maintaining certain elements that identify it as the same composition.

\emph{E-tude I} is based on the first madrigal of Gesualdo's \emph{Sixth Book of Madrigals} called \emph{Se la mia morte brami}. In \emph{E-tude I}, `piano noise' is filtered in different ways using various subtractive synthesis algorithms. In \emph{E-tude I}, several algorithms take the live electronics performer's voice reading the words of the madrigal as input to filter the `piano noise' using different filtering techniques. The most prominent filtering techniques using the voice as input are vocoding (using a variation of the `classic' vocoder algorithm) and the FFTFilter described in the previous chapter.\footnote{See \hyperlink{fftfilter}{pp. 98--99}.} The microphone signal is also used for onset detection, and the live electronics performer may trigger different percussive sounds (generated by filtering bursts of `piano noise' at different frequency ranges) with his/her voice.  Spectral gating (FFT technique which ignores the frequency bins which have magnitudes bellow a certain threshold) of a limited frequency range of the `piano noise' is another technique that is used to isolate the strongest frequencies in a specified range---the resulting frequencies are used as pitched material that is presented either in its natural sinusoidal quality (prominently in the high frequencies) or these frequencies are also mapped into Midi notes which trigger the mechanical piano (prominently in its lower range). At the same time, all pitched material (including the center frequency of some filtered sounds) is altered or defined by the Midi note information received from the pianists performing the madrigal (tuned in \emph{just intonation}). The dynamics for these sounds are also shaped by the Midi velocity from the performance of the Gesualdo. At the same time, the different layers of sound are modified such that they have a similar phrase structure to the madrigals---the layers start and end at the same point in time were the madrigal's phrases do.\footnote{I use this technique in many of my compositions: I plunder the phrase structure of an existing composition to generate a blueprint for a new composition. See \hyperlink{macroplunder}{pp. 89--90}. I also wrote a computer application that automizes this process and generates a visual representation of phrase structure using a Midi File as an input. See \hyperlink{scorevisual}{pp. 109--112}.} The Midi note information (mostly \emph{note-on} messages, but not exclusively) at times also trigger different sounds generated through a combination of filtered `piano noise' and data derived from analysis of instrumental and vocal recordings. A common technique I use in \emph{E-tudes} to make synthetic sounds sound more imperfect or `natural' is to modify the synthesized sound according to the plundered fundamental frequency of an appropriated recorded sound---by altering the frequency and amplitude of synthetic sound according the fundamental of the recorded sound, the synthetic sound becomes more irregular and therefore sounds more `natural' due to the imperfection it inherits from the plundered sound. Another technique I use to make synthesis algorithms sound more `instrumental' is by deriving harmonic structures (the fundamental and partials of a sound) from FFT analysis of appropriated recordings of the instruments I want to approximate. In \emph{E-tude I}, I generate sounds using these techniques to approximate sounds with similar characteristics to a celesta, several percussion instruments, to a vocal melody, high bowed string harmonics, etc., however always using the `piano noise' as the main noise source to be filtered. I also use FFT to process sounds using a combination of spectra between the instrumental recordings and the `piano noise'. \emph{E-tude I} starts with a fairly `abstract' sound world---further removed from recognizable `acoustic' instruments or `traditional' music genres which includes sounds that are less reliant on pitch and more on noise---and very gradually transforms into a different, more `referencial' sound world---which includes pitched sounds, more periodic rhythms, more identifiable sounds modeled on existing instruments and pitch material that is more referential to other more recognizable musical genres. The process of transformation culminates with the emergence of a prominent melody (driven by the FFTFilter, filtering `piano noise' and following the melodic contour of a plundered recording of a vocal sound) and with music that is more reminiscent of a song (or aria) with a rhythmic \emph{ostinato}  using simple (or `primitive') rhythms.

\emph{E-tude II} uses \emph{Belt\`{a} poi che t'assenti} (the second madrigal in the series) as the control structure of the computer processing. In this `e-tude' the role of the `piano noise' is reduced only to a source of noise within physical modeling synthesis algorithms, which create sounds reminiscent to instrumental sounds. These sounds include the high frequency sound based on a physical model of a bowed string which ended \emph{E-tude I}, this time however changing its pitch more gradually and gliding through what sounds as a variation of the main melody of the `song' which \emph{E-tude I} culminates in. They also include sounds reminiscent of plucked strings, percussion and wind instruments all based on physical models of Korean instruments. The sounds reminiscent to plucked strings, for instance, are generated through a variation of the Karplus-Strong plucked-string algorithm using a burst of `piano noise' as its excitation---the result is also altered in frequency through the fundamental frequency of a recording of a plucked \emph{Geomungo} (Korean instrument) string and in amplitude by the amplitude envelope of the same recording. In addition to the sounds generated by the synthesis algorithms, the mechanical piano produces pitched material consisting on ascending \emph{arpeggios} generated through the \emph{synthrumentation} of vowel sounds that the live electronics performer produces into the microphone.\footnote{The PartialTracker class is used to detect the strongest frequencies of these vowel sounds to generate the notes of the \emph{arpeggios}, see \hyperlink{partrack}{p. 97}.} The prominent melodic sound which emerges on its own at around the middle of \emph{E-tude II}, is generated through a combination of `classic' filtering techniques (band pass, high pass, dynamic bank of resonators, etc.) which filter the `piano noise' to try to emulate a `wind instrument' with vocal characteristics. The attack and release envelopes of this sound are derived from a recording of a recorder---the release of the sound sometimes comes with a high-frequency noise reminiscent of human breathing (the frequency range of the noise was mapped approximately to the frequency range of a person's breathing-in sound). This `virtual instrument' plays a melody that is plundered from a recording of \emph{Gagok}---a traditional from of Korean vocal music.\footnote{See \hyperlink{rockwell}{Rockwell (1972)}, for an introduction to \emph{Gagok}.} After a brief solo section, an ensemble of `virtual instruments' joins the melodic sound and emulates music reminiscent of \emph{Gagok}---two other `virtual instruments' join in to play the same melody (one of them sounds more `sinusoidal' and the other sound closer to a reed instrument), but with slight melodic deviations in timing and tuning. These deviations are driven by a generative algorithm (meaning that each time that \emph{E-tude II} is performed, the melodic deviations vary) that is designed to emulate the types of improvised melodic deviations found in original \emph{Gagok}. During the duration of \emph{E-tudes II}, the rate at which sporadic events (plucked strings, percussion instruments and mechanical piano \emph{arpeggios}) take place, gradually becomes faster and the events themselves become more active and unpredictable (for instance the mechanical piano \emph{arpeggios} become faster and with more notes and their direction starts changing randomly as they become more active), until they are squashed against each other and become cluttered at the end of the composition.

\emph{E-tude III} is based on the third madrigal by Gesualdo called \emph{Tu piangi, o filli mia}. This `e-tude' opposes synthesized pitched sounds with a strong fundamental frequency (some of them are even very close to being sinusoidal) at the beginning, with the `piano noise' that is revealed for the fist time in its unfiltered and unprocessed form for the first time later in the composition. The pitched sounds consist mainly of three different kinds of sounds: two sinusoidal pitches, `celesta' sounds and a single `plucked string' descending \emph{arpeggio}. The two sinusoidal pitches gradually change in frequency, first gradually detuning away from each other and later changing direction until they slowly get closer to each other, producing \emph{beating} before merging into the same tone. The `celesta' sounds are generated the same way as the ones that were used in \emph{E-tude I} (which are generated from filtered `piano noise') and derive their pitch material from the spectral data (\emph{synthrumentation}) of the audio signal of just one of the pianists playing \emph{\'{e}tudes} and their rhythm from onset detection of the live microphone signal (the voice of the live electronics performer reading the text of the madrigal). The descending \emph{arpeggio} of `plucked string' sounds---which happens only ones and is reminiscent of the mechanical piano \emph{arpeggios} in \emph{E-tude II}--- are generated through the same Karplus-Strong algorithm used in \emph{E-tude II}, but instead of using a plundered recording a \emph{Geomungo}), uses a recording of a harp. The `piano noise' is first revealed as short percussive sounds (the `piano noise' is filtered so that it sounds like footsteps) triggered by the live electronics performer with the Midi controller at periodic intervals. Then it is revealed as high frequency noise and later as low frequency noise---these noises come from selected bins of an FFT of the `piano noise'. In \emph{E-tude III}, there is a slow transition between pitched sounds which gradually drop out and `piano noise' which gradually becomes dominant as the spectrum is gradually filled by FFT bins randomly---culminating in the complete frequency spectrum of `piano noise' being diffused equally over the speakers. At the end of \emph{E-tudes III}, the `piano noise' vanishes completely in a matter of seconds (through the reverse FFT process) followed by silence with the exception of a series of sporadic and aggressive bursts of clusters by the mechanical piano and by short phrases of sinusoidal sounds and \emph{synthurmentized} piano chords revealing harmonies from the \emph{\'{e}tudes}. 

\emph{E-tude IV} derives information from three main sources: Gesualdo's forth madrigal called \emph{Resta di Darmi Noia}, the \emph{\'{e}tudes} chosen by the pianists and several appropriated recordings of Pygmy music.\footnote{\hyperlink{pygmy}{Mbuti Pygmies of Ituri Rainforest (1992)}.} Pitched material is derived from the Pygmy music recordings using the SpearToMIDI and PartialTracker classes described in the previous chapter.\footnote{See \hyperlink{spectrack}{pp. 95--103}.} The results from the different types of analysis are then selected and combined according to the desired musical outcome and is stored as Midi note data. The collected Midi data is then modified and transformed through the incoming Midi information from the Gesualdo and the spectral data derived from the analysis of the \emph{\'{e}tudes}. The Midi information resulting from this process is realized by the mechanical piano, which is the only source that produces sound, becoming the `virtual' soloist of \emph{E-tude IV}. The spectral data of the \emph{\'{e}tudes} is combined with the collected data from the Pygmy music recordings and the result is modified by the melodic and harmonic material from the madrigal. At some points, the melodic contour from the different voices from the Gesualdo is used to modify the tempo of the different layers of the Midi note data derived from the Pygmy music---if the shape of melody goes up, the tempo progressively becomes faster and if the melody goes down, the tempo becomes slower. In addition, certain algorithmic composition strategies are used to process the final result---for example, stochastic methods are used to control note density by gradually filtering notes in and out. \emph{E-tude IV} uses data of three recordings of Pygmy music, which at the same time represent three sections of the composition. The first section of \emph{E-tude IV} is based on a song for the \emph{molimo} ritual,\footnote{Ibid., Track 24.} which is celebrated on special occasions like for instance after the death an important member of the tribe and is meant to wake the forest up as it seems to be asleep as bad things are happening to its children.\footnote{See \hyperlink{mukenge}{Mukenge (2002)}.} The middle section is based on a recording of a musical bow played by a \emph{Mbuti} pygmy,\footnote{\hyperlink{pygmy}{Mbuti Pygmies of Ituri Rainforest (1992)}, Track 15.} which morphs into the last section that is derived from a recording of hunters signaling, shouting and beating.\footnote{Ibid., Track 5.}

\emph{E-tudes} is a set of compositions that explores a type of live electronics performance that attempts to establish new relationships between performer, composer and audience. It establishes at different times both \emph{interactive} and \emph{interpassive} relationships between the performer and the technological objects as well as between the musicians and the audience.\footnote{See \hyperlink{relaudience}{pp. 46--52}.} It also seeks to establish new forms of exchange between composer and performers as well as between the performers within an ensemble.\footnote{See \hyperlink{ensembledy}{p. 53}.} Furthermore, \emph{E-tudes} combines the use of live electronics, improvisation, real-time computation and generative music to have a result that is unfixed, responsive and which changes for each performance of the work.\footnote{See \hyperlink{techcomp}{pp. 53--56}.} Additionally, \emph{E-tudes} attempts to approach the process of appropriation in new and innovative ways and uses idiosyncratic musical strategies to do so. First, it plunders live performances of existing music, using their audio and Midi signals as building blocks for multiple musical results, therefore promoting the notions of \emph{real-time plunderphonics} and \emph{live musica derivata}.\footnote{See \hyperlink{realtimeplunderfuck}{pp. 91--92}.} It also appropriates \emph{micro} and \emph{macro} elements of notated material, recordings and live signals as well as it treats the musical sources at varying degrees of processing, therefore affecting our ability to recognize the source and our perception of its level of musical `abstraction'. Finally, the sources that \emph{E-tudes} appropriates from, unlike classic \emph{plunderphonics}, come from less familiar sources that might be perceived as more obscure or exotic: not very well known music form places far removed from western culture (Korean traditional vocal music, Pygmy Music), early music that is not performed very often (Gesualdo madrigals) and pop music that is not desired by the mainstream of consumer culture.\footnote{See \hyperlink{appropstrat}{pp. 88--91}.}

\section{On Violence}
 
 \emph{On Violence}\footnote{See \hyperlink{portfolio}{Contents of Portfolio}: CD I (\emph{Compositions}), Track 5 and DVD II (\emph{On Violence}).} is a composition for piano, live electronics, sensors and computer display.\footnote{The code of the programmes that run \emph{On Violence} can be found at \href{http://github.com/freuben/OnViolence}{\texttt {http://github.com/freuben/OnViolence}}.} The pianist reads from a score displayed on the laptop screen, which combines conventional notation with other real-time scoring strategies previously discussed.\footnote{See \hyperlink{realtimescore}{pp. 103--104}.} The performer also wears headphones to receive audio triggers and cues that constitute the aural element of the score. I implemented this score using SuperCollider and the AlgorithmicScore class.\footnote{See \hyperlink{algoscore}{pp. 104--109}.} The pianist interacts with the score through two midi pedals that are used to `turn pages', display graphic notation, give written directions to the performer and activate score animations. During the duration of the performance, the score gradually progresses from conventional notation to more experimental notations that the pianist needs to respond to within the immediacy of the performance.\footnote{See DVD II (\emph{On Violence}) \tiny \textgreater \footnotesize \hspace{0pt} Documentation \tiny \textgreater \footnotesize \hspace{0pt} Performance Materials \tiny \textgreater \footnotesize \hspace{0pt} Score Demo, for a demo of how the score might look/sound during performance.} The real-time scoring elements of \emph{On Violence} use a combination of chance, generative and spectral methods to generate visual and aural material that changes and adapts to each performance. The pianist therefore is asked to follow a score that has both fixed and unfixed indications, some of them involving spontaneous reaction and improvisation. Some of the real-time information that is generated by the computer and thrown at the pianist is very difficult, if not impossible to perform accurately considering that the pianist does not know what s/he will be performing, but nevertheless it is asked to do his/her best to perform them. This is done deliberately to explore the notion of of establishing an \emph{interpassive} relationship between the technological object (the computer displaying the score) and the performer---the pianist becomes frantically active by the `impossible' demands from the technological object which remains passive.\footnote{See \hyperlink{zizekinterpassiv}{pp. 48--49}.}

\emph{On Violence} appropriates existing music form various sources: Dieterich Buxtehude's \emph{Praeludium in G Minor, BuxWV 132}, Einst\"{u}rzende Neubauten's \emph{Autobahn} and  Tristan Wagner's \emph{Parsifal} and \emph{Tristan und Isolde} and plunders sounds like political speeches, screams and metal banging. Buxtehude's \emph{Praeludium} serves as a blueprint for the whole composition---the other derived existing music is placed within its form, and is treated and processed through its shapes and contours as well as its melodic, harmonic and counterpointal content. The live electronics part is triggered by the audio analysis of the music performed by the pianist (a microphone is placed close to the piano for real-time analysis) and by the Midi pedals. Through a combination of machine listening technology and by tracking the incoming data from the Midi pedals the computer is able to follow the score, therefore triggering different sounds and types of processing according to the structure of the composition. There are various predominant sounds coming from the live electronics part at the beginning of the composition: metaling bangs, vocal sounds (screams and fragments of political speeches some of which are heavily processed), high frequency distorted sounds and a sound reminiscent to the noise produced by a motor. The metal bangs are triggered through the onset detection of the piano signal and are stretched or shortened in real-time depending on the speed at which the pianist plays. The samples of vocal sounds and the high frequency distorted sounds are selected randomly and triggered at specific moments according to the speed of the pianists playing and are also stretched or shortened depending on the tracked tempo. Some of the vocal sounds are processed by two different types of synthesis algorithms: one of them convolves two different types of weakly nonlinear oscillators which use the sample as its external force and the other convolves one weakly nonlinear oscillator with the result of a linear predictive coding (LPC) error.\footnote{Part of the SuperCollider SLUGens library by Nick Collins.} The high frequency sounds are spectrally gated samples of guitar feedback that are passed through a distortion guitar pedal. The sound reminiscent of a motor is turned on and off through the Midi pedals and consists of a low frequency pulse wave that is distorted through an overdrive guitar pedal. The pianist controls the frequency of the pulse wave with two 3G Force sensors (accelerometers) attached to his hands, which track arm movement (as the pianist lifts his/her arms, the frequency increases and as s/he lowers them, the frequency decreases). During the first section of the composition, the pianist alternates between incessant banging of chords at a periodic rhythm and controlling the `motor' sound by lifting and lowering his/her arms. The chords that s/he plays are derived from the spectral analysis (using the PartialTracker class) of Neubauten's song \emph{Autobahn} and modified both in harmonic material and register by the Buxtehude. The pitched material of the vocal and distorted high frequency sounds as well as the `motor' sound are also modified in pitch through the Buxtehude score. As the pianist's score starts to incorporate more real-time scoring elements the content of the music also gradually starts to transform. Samples of plundered recordings of Wagner's \emph{Parsifal} and \emph{Tristan und Isolde} start to emerge, however modified in their playing rate by the mapped shapes of Buxtahude's counterpoint. The Wagner samples become more prominent as the pianist is asked to improvise and react immediately to the algorithmic score, culminating in a solo electronic solo that is produced by a synthesis algorithm emulating an organ sound and playing notes derived from a recording of the corresponding section of the Buxtahude. The performer after the electronics solo, responds by playing the next phrase of the original Buxtahude on the piano. The composition ends with the following section were the pianist is asked to improvise freely together with an electronic part that consists of a re-synthesized version of the Prelude to the third act of \emph{Parsifal}. The synthesis algorithm used in the electronics in this section uses a dynamic bank of resonators filtering noise\footnote{I created this noise using the same algorithm I used to generate the `piano noise' in \emph{E-tudes}, however controlling its amplitude with low frequency noise, to emulate bowing.} and modified in frequency and amplitude by the extracted fundamental of the instruments in the original plundered recording, resulting in a sound that approximates a bowed string instrument. This synthesis algorithm is then used to emulate a string orchestra by programming it to perform the parts of the different instruments of the string orchestra (violins, violas, cellos, double basses) and by multiplying its results by the amount of instruments per section and by adding a slight random divination in pitch and timing for each result to imitate the sound of various instruments playing in unison. This version of the Wagner Prelude however is altered in its pitch content to match the previous section and for that reason the notes of the original are modified through the Buxtahude harmonic material. 
 
This composition is inspired by slovenian philosopher Slavoj \v{Z}i\v{z}ek's book called \emph{Violence}.\footnote{See \hyperlink{zizekviolence}{\v{Z}i\v{z}ek (2008)}.}
In this book, \v{Z}i\v{z}ek categorizes violence into two main types: subjective and objective violence. Subjective violence is clearly identifiable by an agent, for example acts of terror or crime, and it is perceived as a clear interruption of the normal state of things. On the other hand, objective violence is violence that is inherent in the social fabric and it is hard to see and experience for the advantaged classes or countries. What \v{Z}i\v{z}ek argues is that objective violence is inherent within the social ``balance'' and it is objective violence which triggers acts of subjective violence. Furthermore, \v{Z}i\v{z}ek identifies two types of objective violence: symbolic and systematic violence. Systematic violence is manifested through our economic and political systems that in order to give the idea of a normal smooth running of things, exert systematic violence on large groups of people. Symbolic violence is related to and included within systematic violence but it is specific to violence expressed through language and other symbolic systems (like music). \v{Z}i\v{z}ek goes further to argue that the forms of symbolic violence are actually based on and manifested by the symbolic systems as such.\footnote{Ibid. pp. 8--63.} \emph{On Violence} therefore attempts to explore the aesthetics of violence and reflect on the different manifestations of violence categorized by \v{Z}i\v{z}ek. It does so, not only through the type of sounds and plundered music it uses (which are suggestive of different types of violence), but by the way in which the performance itself is set up. For instance, the pianist is not only asked to be violent by the sheer force and agressiveness s/he has to execute over the instrument, but also violence is forced upon him/her by the amount of information that is thrown to him/her by technology and by the difficulty of performing all the tasks s/he is asked to. Even though at the beginning of the composition, it seems that the pianist is the one exerting subjective violence onto the piano and controlling technology---by triggering sounds through pedals and controlling the `motor' sound through the sensors---we later find out that it is technology that is controlling the pianist by flooding him/her with impossible demands. At the end of the composition, while the pianist is seemingly free (s/he is asked to improvise freely and play what ever s/he wants) we get the impression that s/he is not the one in control and actually is the victim of objective violence imposed systematically upon him/her not only by the system of performance and technology, but by the symbolic violence implicit in the music itself.

\section{\v{Z}i\v{z}ek!?}

\emph{\v{Z}i\v{z}ek!?}\footnote{See \hyperlink{portfolio}{Contents of Portfolio}: CD I (\emph{Compositions}), Track 6 and DVD III (\emph{\v{Z}i\v{z}ek!?}).} was commissioned for an event about slovenian philosopher Slavoj \v{Z}i\v{z}ek, which took place at The Sound Source, Kings Place, London, July, 2009. I was commissioned to write a live alternative soundtrack to Astra Taylor's \emph{\v{Z}i\v{z}ek!} (2005) documentary, which resulted in a \mbox{computer-mediated} performance for three improvisers playing piano, double bass and drums.\footnote{It was premiered by Alexander Hawkins (piano), Dominic Lash (double bass) and Javier Carmona (drums).} In \emph{\v{Z}i\v{z}ek!?}, each improviser has a laptop in front of them, connected through a computer network by which the improvisers receive individual written directions, timing instructions, score animations (moving graphical notation) and through headphones, each of them receives a different aural score that consists of music they have to interact with.\footnote{See DVD III (\emph{\v{Z}i\v{z}ek!?}) \tiny \textgreater \footnotesize \hspace{0pt} Documentation \tiny \textgreater \footnotesize \hspace{0pt} Performance Materials \tiny \textgreater \footnotesize \hspace{0pt} Score Parts.} The result is a real-time multimedia score that is synchronized with the film and is driven by computer programmes written in SuperCollider,\footnote{The code of the programmes that run \emph{\v{Z}i\v{z}ek!?} can be found at \href{http://github.com/freuben/Zizek}{\texttt {http://github.com/freuben/Zizek}}.} including the AlgorithmicScore class discussed previously.\footnote{See \hyperlink{algoscore}{pp. 104--109}} The improvisers receive different types of written directions through the laptop displays, which all have meanings regarding how they should react during the performance:
\begin{enumerate}
\item \emph{Silence}: Don't play (or stop playing).
\item \emph{Like} : Imitate the music coming from the headphones.
\item \emph{With} : Play together with the music coming from the headphones.
\item \emph{Free}: Free to improvise at will.
\item \emph{Solo}: Improvise a solo.
\item \emph{Unlike}: Play in opposition to the music coming from the headphones. 
\item \emph{Without}: Improvise ignoring the sound from the headphones.
\end{enumerate}
The letters of the written directions are also color coded using the three colors of a stop light: the \emph{Silence} direction is always in \emph{red} meaning they should stop playing at that point, the other directions are displayed in \emph{green} when they should play and \emph{yellow} when the \emph{Silence} direction is about to come. The performers also receive a written indication of which scene of the film is playing at a specific moment in time. The letters of these indications are in \emph{blue} while a specific scene is playing but turn \emph{pink} just before the next scene is about to start. In addition to written directions and indications, the multimedia score also includes score animations, which consist of moving graphical scores that convey some type of activity or gesture.\footnote{See \hyperlink{algoanimation}{pp. 107--108}.} Each score animation is deliberately devised for a specific instrument and is open to interpretation by the performer.\footnote{See DVD III (\emph{\v{Z}i\v{z}ek!?}) \tiny \textgreater \footnotesize \hspace{0pt} Documentation \tiny \textgreater \footnotesize \hspace{0pt} Score Animations, for the interpretation of the animations by the improvisers during the performance.} The written directions, audio score and score animations are triggered through a control structure derived from a Midi file of Johannes Ockeghem's \emph{Missa Mi-mi} (using computer-aided-composition tools described in the previous chapter).\footnote{See \hyperlink{compueraided}{pp. 109--113}.} The music that the aural score is comprised of and to which the improvisers have to react, at the same time is based on different types of analysis and processing of the audio of the film. Therefore, the aural score is not only synchronized to the film's audio, but it is derived from it. The music within the aural score is the result of different types of analysis from the speech, music and other sounds that comprise the film's soundtrack. Pitch and rhythmic material as well as frequency content and dynamics are therefore derived from the analysis of the audio signal of the soundtrack using different techniques, including tempo tracking, onset detection and spectral analysis (PartialTracker and SpearToMidi classes)\footnote{See \hyperlink{spectrack}{pp. 95--103}.} to control synthesized and sampled sounds. Additionally, the audio of the film is sometimes radically processed and used within the audio score. The result is an audio score that while retaining some characteristics of the speech and other sounds from the film, is also allusive to other styles of music as a consequence of the chosen sounds and analysis parameters.\footnote{See DVD III (\emph{\v{Z}i\v{z}ek!?}) \tiny \textgreater \footnotesize \hspace{0pt} Documentation \tiny \textgreater \footnotesize \hspace{0pt} Score Demo.} The way in which the improvisers react to the audio score also produces a musical result that is related to the characteristics of speech and other sounds in the soundtrack of the film and creates and interaction between the musicians and the live sound of the film, which they might carry into their free improvisations.

\emph{\v{Z}i\v{z}ek!?} attempts to find new ways of collaborating with musicians with a background in improvisation by using computer-mediated performance strategies that makes use of their strengths as musicians but at the same time working within a pre-composed structure. By giving the improvisers visual directions and aural stimulus (through headphones), it is possible to direct them towards a certain type of musical behavior and sound world without limiting their creative input in the performance. Furthermore, \emph{\v{Z}i\v{z}ek!?} shares certain characteristics with generative music in that the composer does not specify every detail of the final outcome but a set of possible outcomes that share similar characteristics.\footnote{See \hyperlink{realtimepos}{pp. 54--56}, for a further discussion about the relationship between improvisation and real-time computation, generative music and interactive systems.}

Interpassivity. Appropriation of film, speech, etc.

\section{FreuPinta}

%\subsection{Simulation Series}
%\subsection{Occupation Series}
%\subsection{Transgression Series}

\section{Improvisations}

%\subsection{Horatio Oratorio}
%\subsection{Perro-chimp}
%\subsection{Too Hot to Handel}
%\subsection{Mowgli}

\label{ch:compositions}