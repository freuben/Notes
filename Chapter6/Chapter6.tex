\hypertarget{chapter7}{}
\chapter{Compositions}

This chapter aims to briefly describe the portfolio of submitted work. However, this chapter focuses exclusively on technical and musical descriptions of the musical output and does not have as its purpose to be a commentary on the background, motivation or theoretical framework of the creative process (the previous chapters already serve as a meta-commentary of the submitted work). I will therefore concentrate on . . . .

%During the research period, I have engaged in work that is varied in its outcome: it includes results that maybe are difficult to categorize, but which involve a varying amount of pre-compositional work. Some of my creative output therefore may be categorized as compositions, improvisations, performances, studies or spontaneous experiments.

\section{E-tudes}

\emph{E-tudes}\footnote{See \hyperlink{portfolio}{Contents of Portfolio}: CD I (\emph{Compositions}), Tracks 1--4 and DVD I (\emph{E-tudes}).} is a set of electronic \emph{\'{e}tudes} for six stage pianos, live electronics and mechanical piano.\footnote{In case a mechanical piano is not available, it is possible to use a sampler with piano sounds.} These compositions were written for the ensemble \textbf{piano}\emph{circus}\footnote{See \href{http://www.pianocircus.com/}{\texttt{http://www.pianocircus.com/}}} for a project that became a \mbox{two-year} collaboration and lead to two performances.\footnote{Enterprise 08 Festival, The Space, London, May, 2008, and The Sound Source, Kings Place, London, July, 2009.} What initially attracted me to this ensemble was its very particular instrumentation consisting of six electronic stage pianos. I thought this would be a suitable platform to experiment with the notions of \emph{real-time plunderphonics} and \emph{live musica derivata},\footnote{See \hyperlink{realtimeplunderfuck}{pp. 91--92}.} considering that these instruments are electronic and therefore produce no considerable audible acoustic sound.\footnote{The only acoustic sounds that can be heard are the keyclicks produced by the physical contact with the stage pianos while playing. This noise is slightly audible mostly when there are no sounds playing through the speakers (or they are very quiet).} Like a book of \emph{\'{e}tudes} from the repertoire, \emph{E-tudes} consists of a set of pieces that can be performed together at the same event or individually as separate short pieces. At present time, I have completed four `e-tudes', and as an ongoing project, I will continue adding new pieces to the collection. \emph{E-tudes} is modular in the way in which it can be presented: depending on the set of circumstances for a given event, they can be presented separately or as a whole, either as a concert performance or as an installation with perforative elements. In the installation version, the audience walks into, out of, and around the area surrounding the musicians and has creative control over how they want to experience the performance. By choosing between listening to the speakers in the room or to various headphones that are distributed through the performance space and generate different outputs, each member of the audience fabricates their own version of the piece. Therefore, in the installation version their are various possible outputs generated by the computer from the performance, which the audience can choose from. It is also possible to have a performance were the members of the audience are wearing wireless headphones that can receive multiple channels that are transmitted in the performance space, therefore allowing them to choose which channel they want to listen to.\footnote{This was the case in the performance at Kings Place.}

I use the same configuration for all of the pieces that comprise \emph{E-tudes}: the ensemble of six stage pianos is placed in hexagonal formation and divided into two subgroups. The first subgroup consisting of three pianists are asked to select \emph{\'{e}tudes} from the western piano repertoire at will---they can select the \emph{\'{e}tudes} they prefer to perform (for example, \emph{\'{e}tudes} by Chopin, Debussy or Ligeti, to mention just a few)---and are to play them in their chosen order during the duration of the performance. The second subgroup consisting of the remaining three pianists perform together from \emph{The Sixth Book of Madrigals} by Don Carlo Gesualdo da Venosa (1566-1613). 

The pianists playing the madrigals send Midi information to a computer that transforms the audio signal from the \emph{\'{e}tudes} and schedules the digital signal processing events. The audience is not be able to hear in the room what the pianists are playing as the stage pianos do not produce an acoustic sound. The seventh performer (performing the live-electronic part)\footnote{In the performances of \emph{E-tudes} I performed this part myself.} performs different tasks: at some points s/he speaks the Madrigals' text into a microphone and the spectral information from this signal is be used to process the final audio output and to trigger other sound events, at the same time playing Midi controllers. The live electronic part is not fixed, leaving space for improvisational elements within the human/computer interaction. Finally, through the analysis of all the inputs the computer sends Midi messages to the mechanical piano, adding yet another element to the performance. In the room the final result of the creative process of combining the simultaneous performances in diverse arrangements is diffused through the speakers. In the installation version, the headphones that are spread through the performance space portray the inner life of the performance sounding in the room and reveal the inner layers of computer processing and the appropriated compositions.

Computer programmes play a vital role in all the elements of \emph{E-tudes} and were written in SuperCollider---some of these prgrammes are discussed in \hyperlink{chapter5}{Chapter 5} but some were exclusively written for \emph{E-tudes}.\footnote{The code for these computer programmes can be found at \href{http://github.com/freuben/Etudes}{\texttt {http://github.com/freuben/Etudes}}.} These programmes are used to analyze incoming Midi data to schedule events\footnote{See \hyperlink{miditrig}{p. 113}.} and for digital signal processing (DSP). The incoming Midi events from the pianists playing Gesualdo is analyzed and divided by each voice of the original madrigals. The computer analysis using score following techniques tracks each voice and according to its position in the score, schedules specific DSP events. The Midi \emph{note} and \emph{velocity} information in some occasions is used to determine certain parameters in the DSP algorithms. The DSP algorithms of the live electronics use as input two mayor audio sources: the input of the combined live audio of the sound generated by the three pianists playing \emph{\'{e}tudes} and \emph{micro} elements\footnote{See \hyperlink{macroplunder}{pp. 89--90}.} derived from various recordings of existing music which I choose to appropriate. The individual live audio signals coming from each pianist playing \emph{\'{e}tudes} are interpolated with one another (by altering the pitch and volume of the signals).\footnote{Each signal is interpolated with the other by gradually pitch-shifting one signal down four octaves and fading out its volume gradually, while at the same time introducing the next signal which would be pitch-shifted four octaves down and gradually transposing it up until its normal pitch, and by gradually fading it in.} The live electronics performer can change the duration of the interpolation between \emph{\'{e}tudes} with a Midi controller. At the same time, the resulting signal is then pitch-shifted again through several pitch ratios (the original signal results in five different signals with varying pitch) generating multiple signals that are then mixed together to create yet another signal. The sounding result of this last signal is a very noisy signal which could be described as `piano noise' (it still retains a piano-like quality). I then utilize this `piano noise' as input in synthesis algorithms which filter it using several techniques. The `piano noise' however is very different to \emph{white noise}, \emph{pink noise} or any other types of noise used in classic synthesis techniques in that its spectral flux is constantly changing and its rate and amount of change is fairly irregular. Additionally, the live electronics performer can change the sonic qualities of the `piano noise'---and therefore also change its spectral flux---by altering the interpolation time of the live signals coming from the pianists playing \emph{\'{e}tudes}. The the first two `e-tudes' in their final result (the final output diffused through the speakers) are composed exclusively using synthesis algorithms which use this `piano noise' as input. At the same time, in the installation version, the audience can listen through headphones to the different outputs at different degrees of processing---for instance, the output of one of the headphone outputs is made out of material generated from the interpolation of \emph{\'{e}tudes}, while another one reveals the `piano noise'. The original appropriated sources (the \emph{\'{e}tudes} and the madrigals by Gesualdo) are also displayed closer to their original form through certain headphone outputs. The algorithms that control the overall process have also generative elements---each time they are performed, they generate slightly different results. The generative characteristics of the algorithms, the varying incoming data from the live performances (the \emph{\'{e}tudes} chosen by the pianists change for each performance of \emph{E-tudes}\footnote{Like Cage's \emph{Imaginary Landscape No.4}, this type of musical appropriation is current, generative and indeterminate. See \hyperlink{landscape4}{p. 81.}, for a further discussion of Cage's appropriation strategy.} and the incoming Midi data from the madrigals varies each time they are performed) as well as the improvisatory elements in the live electronic performer's part, makes \emph{E-tudes} an electronic composition that changes (both in content and performance) each time that it is performed, however maintaining certain elements that identify it as the same composition.

\emph{E-tude I} is based on the first madrigal of Gesualdo's \emph{Sixth Book of Madrigals} called \emph{Se la mia morte brami}. In \emph{E-tude I}, `piano noise' is filtered in different ways using various subtractive synthesis algorithms. In \emph{E-tude I}, several algorithms take the live electronics performer's voice reading the words of the madrigal as input to filter the `piano noise' using different filtering techniques. The most prominent filtering techniques using the voice as input are vocoding (using a variation of the `classic' vocoder algorithm) and the FFTFilter described in the previous chapter.\footnote{See \hyperlink{fftfilter}{pp. 98--99}.} The microphone signal is also used for onset detection, and the live electronics performer may trigger different percussive sounds (generated by filtering bursts of `piano noise' at different frequency ranges) with his/her voice.  Spectral gating (FFT technique which ignores the frequency bins which have magnitudes bellow a certain threshold) of a limited frequency range of the `piano noise' is another technique that is used to isolate the strongest frequencies in a specified range---the resulting frequencies are used as pitched material that is presented either in its natural sinusoidal quality (prominently in the high frequencies) or these frequencies are also mapped into Midi notes which trigger the mechanical piano (prominently in its lower range). At the same time, all pitched material (including the center frequency of some filtered sounds) is altered or defined by the Midi note information received from the pianists performing the madrigal (tuned in \emph{just intonation}). The dynamics for these sounds are also shaped by the Midi velocity from the performance of the Gesualdo. At the same time, the different layers of sound are modified such that they have a similar phrase structure to the madrigals---the layers start and end at the same point in time were the madrigal's phrases do.\footnote{I use this technique in many of my compositions: I plunder the phrase structure of an existing composition to generate a blueprint for a new composition. See \hyperlink{macroplunder}{pp. 89--90}. I also wrote a computer application that automizes this process and generates a visual representation of phrase structure using a Midi File as an input. See \hyperlink{scorevisual}{pp. 109--112}.} The Midi note information (mostly \emph{note-on} messages, but not exclusively) at times also trigger different sounds generated through a combination of filtered `piano noise' and data derived from analysis of instrumental and vocal recordings. A common technique I use in \emph{E-tudes} to make synthetic sounds sound more imperfect or `natural' is to modulate the synthesized sound according to the plundered fundamental frequency of an appropriated recorded sound---by modulating the synthetic sound in frequency and amplitude according the fundamental of the recorded sound, the synthetic sound becomes more irregular and therefore sounds more `natural' due to the imperfection it inherits from the plundered sound. Another technique I use to make synthesis algorithms sound more `instrumental' is by deriving harmonic structures (the fundamental and partials of a sound) from FFT analysis of appropriated recordings of the instruments I want to approximate. In \emph{E-tude I}, I generate sounds using these techniques to approximate sounds with similar characteristics to a celesta, several percussion instruments, to a vocal melody, high bowed string harmonics, etc., however always using the `piano noise' as the main noise source to be filtered. I also use FFT to process sounds using a combination of spectra between the instrumental recordings and the `piano noise'. \emph{E-tude I} starts with a fairly `abstract' sound world---further removed from recognizable `acoustic' instruments or `traditional' music genres which includes sounds that are less reliant on pitch and more on noise---and very gradually transforms into a different, more `referencial' sound world---which includes pitched sounds, more periodic rhythms, more identifiable sounds modeled on existing instruments and pitch material that is more referential to other more recognizable musical genres. The process of transformation culminates with the emergence of a prominent melody (driven by the FFTFilter, filtering `piano noise' and following the melodic contour of a plundered recording of a vocal sound) and with music that is more reminiscent of a song (or aria) with a rhythmic \emph{ostinato}  using simple (or `primitive') rhythms.

\emph{E-tude II} uses \emph{Belt\`{a} poi che t'assenti} (the second madrigal in the series) as the control structure of the computer processing. In this `e-tude' the role of the `piano noise' is reduced only to a source of noise within physical modeling synthesis algorithms, which create sounds reminiscent to instrumental sounds. These sounds include the high frequency sound based on a physical model of a bowed string which ended \emph{E-tude I}, this time however modulating much more in pitch and gliding through what sounds as a variation of the main melody of the `song' which \emph{E-tude I} culminates in. They also include sounds reminiscent of plucked strings, percussion and wind instruments all based on physical models of Korean instruments. The sounds reminiscent to plucked strings, for instance, are generated through a variation of the Karplus-Strong plucked-string algorithm using a burst of `piano noise' as its excitation---the result is also modulated in frequency through the fundamental frequency of a recording of a plucked \emph{Geomungo} (Korean instrument) string and in amplitude by the amplitude envelope of the same recording. In addition to the sounds generated by the synthesis algorithms, the mechanical piano produces pitched material consisting on ascending \emph{arpeggios} generated through the \emph{synthrumentation} of vowel sounds that the live electronics performer produces into the microphone.\footnote{The PartialTracker class is used to detect the strongest frequencies of these vowel sounds to generate the notes of the \emph{arpeggios}, see \hyperlink{partrack}{p. 97}.} The prominent melodic sound which emerges on its own at around the middle of \emph{E-tude II}, is generated through a combination of `classic' filtering techniques (band pass, high pass, dynamic bank of resonators, etc.) which filter the `piano noise' to try to emulate a `wind instrument' with vocal characteristics. The attack and release envelopes of this sound are derived from a recording of a recorder---the release of the sound sometimes comes with a high-frequency noise reminiscent of human breathing (the frequency range of the noise was mapped approximately to the frequency range of a person's breathing-in sound). This `virtual instrument' plays a melody that is plundered from a recording of \emph{Gagok}---a traditional from of Korean vocal music.\footnote{See \hyperlink{rockwell}{Rockwell (1972)}, for an introduction to \emph{Gagok}.} After a brief solo section, an ensemble of `virtual instruments' joins the melodic sound and emulates music reminiscent of \emph{Gagok}---two other `virtual instruments' join in to play the same melody (one of them sounds more `sinusoidal' and the other sound closer to a reed instrument), but with slight melodic deviations in timing and tuning. These deviations are driven by a generative algorithm (meaning that each time that \emph{E-tude II} is performed, the melodic deviations vary) that is designed to emulate the types of improvised melodic deviations found in original \emph{Gagok}. During the duration of \emph{E-tudes II}, the rate at which sporadic events (plucked strings, percussion instruments and mechanical piano \emph{arpeggios}) take place, gradually becomes faster and the events themselves become more active and unpredictable (for instance the mechanical piano \emph{arpeggios} become faster and with more notes and their direction starts changing randomly as they become more active), until they are squashed against each other and become cluttered at the end of the composition.

\emph{E-tude III} is based on the third madrigal by Gesualdo called \emph{Tu piangi, o filli mia}. This `e-tude' opposes synthesized pitched sounds with a strong fundamental frequency (some of them are even very close to being sinusoidal) at the beginning, with the `piano noise' that is revealed for the fist time in its unfiltered and unprocessed form for the first time later in the composition. The pitched sounds consist mainly of three different kinds of sounds: two sinusoidal pitches, `celesta' sounds and a single `plucked string' descending \emph{arpeggio}. The two sinusoidal pitches gradually change in frequency, first gradually detuning away from each other and later changing direction until they slowly get closer to each other, producing \emph{beating} before merging into the same tone. The `celesta' sounds are generated the same way as the ones that were used in \emph{E-tude I} (which are generated from filtered `piano noise') and derive their pitch material from the spectral data (\emph{synthrumentation}) of the audio signal of just one of the pianists playing \emph{\'{e}tudes} and their rhythm from onset detection of the live microphone signal (the voice of the live electronics performer reading the text of the madrigal). The descending \emph{arpeggio} of `plucked string' sounds---which happens only ones and is reminiscent of the mechanical piano \emph{arpeggios} in \emph{E-tude II}--- are generated through the same Karplus-Strong algorithm used in \emph{E-tude II}, but instead of using a plundered recording a \emph{Geomungo}), uses a recording of a harp. The `piano noise' is first revealed as short percussive sounds (the `piano noise' is filtered so that it sounds like footsteps) triggered by the live electronics performer with a Midi controller at periodic intervals. Then it is revealed as high frequency noise and later as low frequency noise---these noises come from selected bins of an FFT of the `piano noise'. In \emph{E-tude III}, there is a slow transition between pitched sounds which gradually drop out and `piano noise' which gradually becomes dominant as the spectrum is gradually filled by FFT bins randomly---culminating in the complete frequency spectrum of `piano noise' being diffused equally over the speakers. At the end of \emph{E-tudes III}, the `piano noise' vanishes completely in a matter of seconds (through the reverse FFT process) followed by silence with the exception of a series of sporadic and aggressive bursts of clusters by the mechanical piano and by short phrases of sinusoidal sounds and \emph{synthurmentized} piano chords revealing harmonies from the \emph{\'{e}tudes}. 

\emph{E-tude IV} derives information from three main sources: Gesualdo's forth madrigal called \emph{Resta di Darmi Noia}, the \emph{\'{e}tudes} chosen by the pianists and several appropriated recordings of Pygmy music.\footnote{\hyperlink{pygmy}{Mbuti Pygmies of Ituri Rainforest (1992)}.} Pitched material is derived from the Pygmy music recordings using the SpearToMIDI and PartialTracker classes described in the previous chapter.\footnote{See \hyperlink{spectrack}{pp. 95--103}.} The results from the different types of analysis are then selected and combined according to the desired musical outcome and is stored as Midi note data. The collected Midi data is then modified and transformed through the incoming Midi information from the Gesualdo and the spectral data derived from the analysis of the \emph{\'{e}tudes}. The Midi information resulting from this process is realized by the mechanical piano, which is the only source that produces sound, becoming the `virtual' soloist of \emph{E-tude IV}. The spectral data of the \emph{\'{e}tudes} is combined with the collected data from the Pygmy music recordings and the result is modified by the melodic and harmonic material from the madrigal. At some points, the melodic contour from the different voices from the Gesualdo is used to modify the tempo of the different layers of the Midi note data derived from the Pygmy music---if the shape of melody goes up, the tempo progressively becomes faster and if the melody goes down, the tempo becomes slower. In addition, certain algorithmic composition strategies are used to process the final result---for example, stochastic methods are used to control note density by gradually filtering notes in and out. \emph{E-tude IV} uses data of three recordings of Pygmy music, which at the same time represent three sections of the composition. The first section of \emph{E-tude IV} is based on a song for the \emph{molimo} ritual,\footnote{Ibid., Track 24.} which is celebrated on special occasions like for instance after the death an important member of the tribe and is meant to wake the forest up as it seems to be asleep as bad things are happening to its children.\footnote{See \hyperlink{mukenge}{Mukenge (2002)}.} The middle section is based on a recording of a musical bow played by a \emph{Mbuti} pygmy,\footnote{\hyperlink{pygmy}{Mbuti Pygmies of Ituri Rainforest (1992)}, Track 15.} which morphs into the last section that is derived from a recording of hunters signaling, shouting and beating.\footnote{Ibid., Track 5.}

\emph{E-tudes} is a set of compositions that explores a type of live electronics performance that attempts to establish new relationships between performer, composer and audience. It establishes both \emph{interactive} and \emph{interpassive} relationships between the performer and the technological objects as well as between the musicians and the audience.\footnote{See \hyperlink{relaudience}{pp. 46--52}.} It also seeks to establish new forms of exchange between composer and performers as well as between the performers within an ensemble.\footnote{See \hyperlink{ensembledy}{p. 53}.} Furthermore, \emph{E-tudes} combines the use of live electronics, improvisation, real-time computation and generative music to have a result that is unfixed, responsive and which changes for each performance of the work.\footnote{See \hyperlink{techcomp}{pp. 53--56}.} 

Additionally, \emph{E-tudes} approaches the process of appropriation by plundering live performances of existing music and using their audio and Midi signals as building blocks for multiple musical results. 

%At the same time, it appropriates \emph{micro} and \emph{macro} elements of notated material, recordings and live signals that are used within a computer programme 
%\footnote{See \hyperlink{elaborationapprop}{pp. 87--93}, for a further discussion on some of my ideas about musical strategies of appropriation.}


\section{On Violence}

This composition attempts to explore the aesthetics of violence and reflect on different manifestations of violence. It is also inspired by slovenian philosopher Slavoj Zizek's ideas about violence. Zizek  categorizes violence into two main types: subjective and objective violence. Subjective violence is clearly identifiable by an agent, for example acts of terror or crime, and it is perceived as a clear interruption of the normal state of things. On the other hand, objective violence is  violence that is inherent in the social fabric and it is hard to see and experience for the advantaged classes or countries. What Zizek argues is that objective violence is inherent within the social "balance" and it is objective violence which triggers acts of subjective violence. Furthermore, Zizek identifies two types of objective violence: symbolic and systematic violence. Systematic violence is manifested through our economic and political systems that in order to give the idea of a normal smooth running of things, exert systematic violence on large groups of people. Symbolic violence is related to and included within systematic violence but it is specific to violence expressed through language and other symbolic systems (like music). Zizek goes further to argue that the forms of symbolic violence are actually based on and manifested by the symbolic systems as such. 

\section{\v{Z}i\v{z}ek!?}

Zizek? is a computer-mediated improvisation that gives a live alternative soundtrack to the Zizek! (2005) movie. Each performer has a laptop in front of them. The laptops are connected through a network by which the composer guides the improvisers by sending them written directions, animations (moving graphical notation) and through headphones, an aural score that consists of sound and music derived from the audio of the film. 

Alexander Hawkins (piano), Dominic Lash (double bass), Javier Carmona (drums)

\section{FreuPinta}

%\subsection{Simulation Series}
%\subsection{Occupation Series}
%\subsection{Transgression Series}

\section{Improvisations}

%\subsection{Horatio Oratorio}
%\subsection{Perro-chimp}
%\subsection{Too Hot to Handel}
%\subsection{Mowgli}

\label{ch:compositions}