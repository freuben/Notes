\hypertarget{chapter7}{}
\chapter{Compositions}

This chapter aims to briefly describe the portfolio of submitted work. However, this chapter focuses exclusively on technical and musical descriptions of the musical output and does not have as its purpose to be a commentary on the background, motivation or theoretical framework of the creative process (the previous chapters already serve as a meta-commentary of the submitted work). During this research period, I have engaged in work that is varied in its outcome: it includes results that maybe are difficult to categorize and which involves varying musical practices. Some of my creative output therefore may be categorized as compositions, improvisations, performances, studies or spontaneous experiments. In this chapter, I will attempt to describe the submitted work briefly with the purpose of giving specific information about the main projects I undertook during the period of research. First, I will describe \emph{E-tudes}, a work that can be presented as a performance or as an installation with performative elements. I will focus on explaining how . . .

\section{E-tudes}

\emph{E-tudes}\footnote{See \hyperlink{portfolio}{Contents of Portfolio}: CD I (\emph{Compositions}), Tracks 1--4 and DVD I (\emph{E-tudes}).} is a set of electronic \emph{\'{e}tudes} for six stage pianos, live electronics and mechanical piano.\footnote{In case a mechanical piano is not available, it is possible to use a sampler with piano sounds.} These compositions were written for the ensemble \textbf{piano}\emph{circus}\footnote{See \href{http://www.pianocircus.com/}{\texttt{http://www.pianocircus.com/}}} for a project that became a \mbox{two-year} collaboration and lead to two performances.\footnote{Enterprise 08 Festival, The Space, London, May, 2008, and The Sound Source, Kings Place, London, July, 2009.} What initially attracted me to this ensemble was its very particular instrumentation consisting of six electronic stage pianos. I thought this would be a suitable platform to experiment with the notions of \emph{real-time plunderphonics} and \emph{live musica derivata},\footnote{See \hyperlink{realtimeplunderfuck}{pp. 91--92}.} considering that these instruments are electronic and therefore produce no considerable audible acoustic sound.\footnote{The only acoustic sounds that can be heard are the keyclicks produced by the physical contact with the stage pianos while playing. This noise is slightly audible mostly when there are no sounds playing through the speakers (or they are very quiet).} Like a book of \emph{\'{e}tudes} from the repertoire, \emph{E-tudes} consists of a set of pieces that can be performed together at the same event or individually as separate short pieces. At present time, I have completed four `e-tudes', and as an ongoing project, I will continue adding new pieces to the collection. \emph{E-tudes} is modular in the way in which it can be presented: depending on the set of circumstances for a given event, they can be presented separately or as a whole, either as a concert performance or as an installation with perforative elements. In the installation version, the audience walks into, out of, and around the area surrounding the musicians and has creative control over how they want to experience the performance. By choosing between listening to the speakers in the room or to various headphones that are distributed through the performance space and generate different outputs, each member of the audience fabricates their own version of the piece. Therefore, in the installation version their are various possible outputs generated by the computer from the performance, which the audience can choose from. It is also possible to have a performance were the members of the audience are wearing wireless headphones that can receive multiple channels that are transmitted in the performance space, therefore allowing them to choose which channel and output from the performance they want to listen to.\footnote{This was the case in the performance at Kings Place.}

I use the same configuration for all of the pieces that comprise \emph{E-tudes}: the ensemble of six stage pianos is placed in hexagonal formation and divided into two subgroups. The first subgroup consisting of three pianists are asked to select \emph{\'{e}tudes} from the western piano repertoire at will---they can select the \emph{\'{e}tudes} they prefer to perform (for example, \emph{\'{e}tudes} by Chopin, Debussy or Ligeti, to mention just a few)---and are to play them in their chosen order during the duration of the performance. The second subgroup consisting of the remaining three pianists perform together from \emph{The Sixth Book of Madrigals} by Don Carlo Gesualdo da Venosa (1566-1613). The pianists playing the madrigals send Midi information to a computer that transforms the audio signal from the \emph{\'{e}tudes} and schedules the computer processing events. The audience is not be able to hear in the room what the pianists are playing as the stage pianos do not produce an acoustic sound. The seventh performer (performing the live electronics part)\footnote{In the performances of \emph{E-tudes} I performed this part myself.} performs different tasks: at some points s/he speaks the Madrigals' text into a microphone and the spectral information from this signal is used to process the final audio output and to trigger other sound events, and at other times s/he mixes the resulting sounds, controls different parameters in the processing and  triggers sounds with a Midi controller. The live electronics part is not fixed, leaving space for improvisational elements within the human/computer interaction. Finally, through the analysis of all the inputs the computer sends Midi messages to the mechanical piano, adding yet another element to the performance. In the room the final result of the creative process of combining the simultaneous performances in diverse arrangements is diffused through the speakers. In the installation version, the headphones that are spread through the performance space portray the inner life of the performance sounding in the room and reveal the inner layers of computer processing and the appropriated compositions.

Computer programmes play a vital role in all the elements of \emph{E-tudes} and were written in SuperCollider---some of these programmes are discussed in \hyperlink{chapter5}{Chapter 5} but some were exclusively written for \emph{E-tudes}.\footnote{The code for these computer programmes can be found at \href{http://github.com/freuben/Etudes}{\texttt {http://github.com/freuben/Etudes}}.} These programmes are used to analyze incoming Midi data to schedule events\footnote{See \hyperlink{miditrig}{p. 113}.} and for digital signal processing (DSP). The incoming Midi events from the pianists playing Gesualdo is analyzed and divided by each voice of the original madrigals. The computer analysis using score following techniques tracks each voice and according to its position in the score, schedules specific DSP events. The Midi \emph{note} and \emph{velocity} information in some occasions is used to determine certain parameters in the DSP algorithms. The DSP algorithms of the live electronics use as input two mayor audio sources: the input of the combined live audio of the sound generated by the three pianists playing \emph{\'{e}tudes} and \emph{micro} elements\footnote{See \hyperlink{macroplunder}{pp. 89--90}.} derived from various recordings of existing music which I choose to appropriate. The individual live audio signals coming from each pianist playing \emph{\'{e}tudes} are interpolated with one another (by altering the pitch and volume of the signals).\footnote{Each signal is interpolated with the other by gradually pitch-shifting one signal down four octaves and fading out its volume gradually, while at the same time introducing the next signal which would be pitch-shifted four octaves down and gradually transposing it up until its normal pitch, and by gradually fading it in.} The live electronics performer can change the duration of the interpolation between \emph{\'{e}tudes} with the Midi controller. At the same time, the resulting signal is then pitch-shifted again through several pitch ratios (the original signal results in five different signals with varying pitch) generating multiple signals that are then mixed together to create yet another signal. The sounding result of this last signal is a very noisy signal which could be described as `piano noise' (it still retains a piano-like quality). I then utilize the `piano noise' as input in synthesis algorithms which filter it using several techniques. The `piano noise' however is very different to \emph{white noise}, \emph{pink noise} or any other types of noise used in classic synthesis techniques in that its spectral flux is constantly changing and its rate and amount of change is fairly irregular. Additionally, the live electronics performer can change the sonic qualities of the `piano noise'---and therefore also change its spectral flux---by altering the interpolation time of the live signals coming from the pianists playing \emph{\'{e}tudes}. Some of the `e-tudes' in their final result (the final output diffused through the speakers) are composed exclusively using synthesis algorithms which use this `piano noise' as input. At the same time, in the installation version, the audience can listen through headphones to the different outputs at different degrees of processing---for instance, one of the headphone outputs is made out entirely of material generated from the interpolation of \emph{\'{e}tudes}, while another one reveals the `piano noise'. The original appropriated sources (the \emph{\'{e}tudes} and the madrigals by Gesualdo) are also displayed closer to their original form through certain headphone outputs. The algorithms that control the overall process have also generative elements---each time they are performed, they generate different results. The generative characteristics of the algorithms, the varying incoming data from the live performances (the \emph{\'{e}tudes} chosen by the pianists change for each performance of \emph{E-tudes}\footnote{Like Cage's \emph{Imaginary Landscape No.4}, this type of musical appropriation is current, generative and indeterminate. See \hyperlink{landscape4}{p. 81.}, for a further discussion of Cage's appropriation strategy.} and the incoming Midi data from the madrigals varies each time they are performed) as well as the improvisatory elements in the live electronics performer's part, makes \emph{E-tudes} an electronic composition that changes (both in content and performance) each time that it is performed, however maintaining certain elements that identify it as the same composition.

\emph{E-tude I} is based on the first madrigal of Gesualdo's \emph{Sixth Book of Madrigals} called \emph{Se la mia morte brami}. In \emph{E-tude I}, `piano noise' is filtered in different ways using various subtractive synthesis algorithms. In \emph{E-tude I}, several algorithms take the live electronics performer's voice reading the words of the madrigal as input to filter the `piano noise' using different filtering techniques. The most prominent filtering techniques using the voice as input are vocoding (using a variation of the `classic' vocoder algorithm) and the FFTFilter described in the previous chapter.\footnote{See \hyperlink{fftfilter}{pp. 98--99}.} The microphone signal is also used for onset detection, and the live electronics performer may trigger different percussive sounds (generated by filtering bursts of `piano noise' at different frequency ranges) with his/her voice.  Spectral gating (FFT technique which ignores the frequency bins which have magnitudes bellow a certain threshold) of a limited frequency range of the `piano noise' is another technique that is used to isolate the strongest frequencies in a specified range---the resulting frequencies are used as pitched material that is presented either in its natural sinusoidal quality (prominently in the high frequencies) or these frequencies are also mapped into Midi notes which trigger the mechanical piano (prominently in its lower range). At the same time, all pitched material (including the center frequency of some filtered sounds) is altered or defined by the Midi note information received from the pianists performing the madrigal (tuned in \emph{just intonation}). The dynamics for these sounds are also shaped by the Midi velocity from the performance of the Gesualdo. At the same time, the different layers of sound are modified such that they have a similar phrase structure to the madrigals---the layers start and end at the same point in time were the madrigal's phrases do.\footnote{I use this technique in many of my compositions: I plunder the phrase structure of an existing composition to generate a blueprint for a new composition. See \hyperlink{macroplunder}{pp. 89--90}. I also wrote a computer application that automizes this process and generates a visual representation of phrase structure using a Midi File as an input. See \hyperlink{scorevisual}{pp. 109--112}.} The Midi note information (mostly \emph{note-on} messages, but not exclusively) at times also trigger different sounds generated through a combination of filtered `piano noise' and data derived from analysis of instrumental and vocal recordings. A common technique I use in \emph{E-tudes} to make synthetic sounds sound more imperfect or `natural' is to modify the synthesized sound according to the plundered fundamental frequency of an appropriated recorded sound---by altering the frequency and amplitude of synthetic sound according the fundamental of the recorded sound, the synthetic sound becomes more irregular and therefore sounds more `natural' due to the imperfection it inherits from the plundered sound. Another technique I use to make synthesis algorithms sound more `instrumental' is by deriving harmonic structures (the fundamental and partials of a sound) from FFT analysis of appropriated recordings of the instruments I want to approximate. In \emph{E-tude I}, I generate sounds using these techniques to approximate sounds with similar characteristics to a celesta, several percussion instruments, to a vocal melody, high bowed string harmonics, etc., however always using the `piano noise' as the main noise source to be filtered. I also use FFT to process sounds using a combination of spectra between the instrumental recordings and the `piano noise'. \emph{E-tude I} starts with a fairly `abstract' sound world---further removed from recognizable `acoustic' instruments or `traditional' music genres which includes sounds that are less reliant on pitch and more on noise---and very gradually transforms into a different, more `referencial' sound world---which includes pitched sounds, more periodic rhythms, more identifiable sounds modeled on existing instruments and pitch material that is more referential to other more recognizable musical genres. The process of transformation culminates with the emergence of a prominent melody (driven by the FFTFilter, filtering `piano noise' and following the melodic contour of a plundered recording of a vocal sound) and with music that is more reminiscent of a song (or aria) with a rhythmic \emph{ostinato}  using simple (or `primitive') rhythms.

\emph{E-tude II} uses \emph{Belt\`{a} poi che t'assenti} (the second madrigal in the series) as the control structure of the computer processing. In this `e-tude' the role of the `piano noise' is reduced only to a source of noise within physical modeling synthesis algorithms, which create sounds reminiscent to instrumental sounds. These sounds include the high frequency sound based on a physical model of a bowed string which ended \emph{E-tude I}, this time however changing its pitch more gradually and gliding through what sounds as a variation of the main melody of the `song' which \emph{E-tude I} culminates in. They also include sounds reminiscent of plucked strings, percussion and wind instruments all based on physical models of Korean instruments. The sounds reminiscent to plucked strings, for instance, are generated through a variation of the Karplus-Strong plucked-string algorithm using a burst of `piano noise' as its excitation---the result is also altered in frequency through the fundamental frequency of a recording of a plucked \emph{Geomungo} (Korean instrument) string and in amplitude by the amplitude envelope of the same recording. In addition to the sounds generated by the synthesis algorithms, the mechanical piano produces pitched material consisting on ascending \emph{arpeggios} generated through the \emph{synthrumentation} of vowel sounds that the live electronics performer produces into the microphone.\footnote{The PartialTracker class is used to detect the strongest frequencies of these vowel sounds to generate the notes of the \emph{arpeggios}, see \hyperlink{partrack}{p. 97}.} The prominent melodic sound which emerges on its own at around the middle of \emph{E-tude II}, is generated through a combination of `classic' filtering techniques (band pass, high pass, dynamic bank of resonators, etc.) which filter the `piano noise' to try to emulate a `wind instrument' with vocal characteristics. The attack and release envelopes of this sound are derived from a recording of a recorder---the release of the sound sometimes comes with a high-frequency noise reminiscent of human breathing (the frequency range of the noise was mapped approximately to the frequency range of a person's breathing-in sound). This `virtual instrument' plays a melody that is plundered from a recording of \emph{Gagok}---a traditional from of Korean vocal music.\footnote{See \hyperlink{rockwell}{Rockwell (1972)}, for an introduction to \emph{Gagok}.} After a brief solo section, an ensemble of `virtual instruments' joins the melodic sound and emulates music reminiscent of \emph{Gagok}---two other `virtual instruments' join in to play the same melody (one of them sounds more `sinusoidal' and the other sound closer to a reed instrument), but with slight melodic deviations in timing and tuning. These deviations are driven by a generative algorithm (meaning that each time that \emph{E-tude II} is performed, the melodic deviations vary) that is designed to emulate the types of improvised melodic deviations found in original \emph{Gagok}. During the duration of \emph{E-tudes II}, the rate at which sporadic events (plucked strings, percussion instruments and mechanical piano \emph{arpeggios}) take place, gradually becomes faster and the events themselves become more active and unpredictable (for instance the mechanical piano \emph{arpeggios} become faster and with more notes and their direction starts changing randomly as they become more active), until they are squashed against each other and become cluttered at the end of the composition.

\emph{E-tude III} is based on the third madrigal by Gesualdo called \emph{Tu piangi, o filli mia}. This `e-tude' opposes synthesized pitched sounds with a strong fundamental frequency (some of them are even very close to being sinusoidal) at the beginning, with the `piano noise' that is revealed for the fist time in its unfiltered and unprocessed form for the first time later in the composition. The pitched sounds consist mainly of three different kinds of sounds: two sinusoidal pitches, `celesta' sounds and a single `plucked string' descending \emph{arpeggio}. The two sinusoidal pitches gradually change in frequency, first gradually detuning away from each other and later changing direction until they slowly get closer to each other, producing \emph{beating} before merging into the same tone. The `celesta' sounds are generated the same way as the ones that were used in \emph{E-tude I} (which are generated from filtered `piano noise') and derive their pitch material from the spectral data (\emph{synthrumentation}) of the audio signal of just one of the pianists playing \emph{\'{e}tudes} and their rhythm from onset detection of the live microphone signal (the voice of the live electronics performer reading the text of the madrigal). The descending \emph{arpeggio} of `plucked string' sounds---which happens only ones and is reminiscent of the mechanical piano \emph{arpeggios} in \emph{E-tude II}--- are generated through the same Karplus-Strong algorithm used in \emph{E-tude II}, but instead of using a plundered recording a \emph{Geomungo}), uses a recording of a harp. The `piano noise' is first revealed as short percussive sounds (the `piano noise' is filtered so that it sounds like footsteps) triggered by the live electronics performer with the Midi controller at periodic intervals. Then it is revealed as high frequency noise and later as low frequency noise---these noises come from selected bins of an FFT of the `piano noise'. In \emph{E-tude III}, there is a slow transition between pitched sounds which gradually drop out and `piano noise' which gradually becomes dominant as the spectrum is gradually filled by FFT bins randomly---culminating in the complete frequency spectrum of `piano noise' being diffused equally over the speakers. At the end of \emph{E-tudes III}, the `piano noise' vanishes completely in a matter of seconds (through the reverse FFT process) followed by silence with the exception of a series of sporadic and aggressive bursts of clusters by the mechanical piano and by short phrases of sinusoidal sounds and \emph{synthurmentized} piano chords revealing harmonies from the \emph{\'{e}tudes}. 

\emph{E-tude IV} derives information from three main sources: Gesualdo's forth madrigal called \emph{Resta di Darmi Noia}, the \emph{\'{e}tudes} chosen by the pianists and several appropriated recordings of Pygmy music.\footnote{\hyperlink{pygmy}{Mbuti Pygmies of Ituri Rainforest (1992)}.} Pitched material is derived from the Pygmy music recordings using the SpearToMIDI and PartialTracker classes described in the previous chapter.\footnote{See \hyperlink{spectrack}{pp. 95--103}.} The results from the different types of analysis are then selected and combined according to the desired musical outcome and is stored as Midi note data. The collected Midi data is then modified and transformed through the incoming Midi information from the Gesualdo and the spectral data derived from the analysis of the \emph{\'{e}tudes}. The Midi information resulting from this process is realized by the mechanical piano, which is the only source that produces sound, becoming the `virtual' soloist of \emph{E-tude IV}. The spectral data of the \emph{\'{e}tudes} is combined with the collected data from the Pygmy music recordings and the result is modified by the melodic and harmonic material from the madrigal. At some points, the melodic contour from the different voices from the Gesualdo is used to modify the tempo of the different layers of the Midi note data derived from the Pygmy music---if the shape of melody goes up, the tempo progressively becomes faster and if the melody goes down, the tempo becomes slower. In addition, certain algorithmic composition strategies are used to process the final result---for example, stochastic methods are used to control note density by gradually filtering notes in and out. \emph{E-tude IV} uses data of three recordings of Pygmy music, which at the same time represent three sections of the composition. The first section of \emph{E-tude IV} is based on a song for the \emph{molimo} ritual,\footnote{Ibid., Track 24.} which is celebrated on special occasions like for instance after the death an important member of the tribe and is meant to wake the forest up as it seems to be asleep as bad things are happening to its children.\footnote{See \hyperlink{mukenge}{Mukenge (2002)}.} The middle section is based on a recording of a musical bow played by a \emph{Mbuti} pygmy,\footnote{\hyperlink{pygmy}{Mbuti Pygmies of Ituri Rainforest (1992)}, Track 15.} which morphs into the last section that is derived from a recording of hunters signaling, shouting and beating.\footnote{Ibid., Track 5.}

\emph{E-tudes} is a set of compositions that explores a type of live electronics performance that attempts to establish new relationships between performer, composer and audience. It establishes at different times both \emph{interactive} and \emph{interpassive} relationships between the performer and the technological objects as well as between the musicians and the audience.\footnote{See \hyperlink{relaudience}{pp. 46--52}.} It also seeks to establish new forms of exchange between composer and performers as well as between the performers within an ensemble.\footnote{See \hyperlink{ensembledy}{p. 53}.} Furthermore, \emph{E-tudes} combines the use of live electronics, improvisation, real-time computation and generative music to have a result that is unfixed, responsive and which changes for each performance of the work.\footnote{See \hyperlink{techcomp}{pp. 53--56}.} Additionally, \emph{E-tudes} attempts to approach the process of appropriation in new and innovative ways and uses idiosyncratic musical strategies to do so. First, it plunders live performances of existing music, using their audio and Midi signals as building blocks for multiple musical results, therefore promoting the notions of \emph{real-time plunderphonics} and \emph{live musica derivata}.\footnote{See \hyperlink{realtimeplunderfuck}{pp. 91--92}.} It also appropriates \emph{micro} and \emph{macro} elements of notated material, recordings and live signals as well as it treats the musical sources at varying degrees of processing, therefore affecting our ability to recognize the source and our perception of its level of musical `abstraction'. Finally, the sources that \emph{E-tudes} appropriates from, unlike classic \emph{plunderphonics}, come from less familiar sources that might be perceived as more obscure or exotic: not very well known music form places far removed from western culture (Korean traditional vocal music, Pygmy Music), early music that is not performed very often (Gesualdo madrigals) and pop music that is not desired by the mainstream of consumer culture.\footnote{See \hyperlink{appropstrat}{pp. 88--91}.}

\section{On Violence}
 
 \emph{On Violence}\footnote{See \hyperlink{portfolio}{Contents of Portfolio}: CD I (\emph{Compositions}), Track 5 and DVD II (\emph{On Violence}).} is a composition for piano, live electronics, sensors and computer display.\footnote{The code of the programmes that run \emph{On Violence} can be found at \href{http://github.com/freuben/OnViolence}{\texttt {http://github.com/freuben/OnViolence}}.} The pianist reads from a score displayed on the laptop screen, which combines conventional notation with other real-time scoring strategies previously discussed.\footnote{See \hyperlink{realtimescore}{pp. 103--104}.} The performer also wears headphones to receive audio triggers and cues that constitute the aural element of the score. I implemented this score using SuperCollider and the AlgorithmicScore class.\footnote{See \hyperlink{algoscore}{pp. 104--109}.} The pianist interacts with the score through two midi pedals that are used to `turn pages', display graphic notation, give written directions to the performer and activate score animations. During the duration of the performance, the score gradually progresses from conventional notation to more experimental notations that the pianist needs to respond to within the immediacy of the performance.\footnote{See DVD II (\emph{On Violence}) \tiny \textgreater \footnotesize \hspace{0pt} Documentation \tiny \textgreater \footnotesize \hspace{0pt} Performance Materials \tiny \textgreater \footnotesize \hspace{0pt} Score Demo, for a demo of how the score might look/sound during performance.} The real-time scoring elements of \emph{On Violence} use a combination of chance, generative and spectral methods to generate visual and aural material that changes and adapts to each performance. The pianist therefore is asked to follow a score that has both fixed and unfixed indications, some of them involving spontaneous reaction and improvisation. Some of the real-time information that is generated by the computer and thrown at the pianist is very difficult, if not impossible to perform accurately considering that the pianist does not know what s/he will be performing, but nevertheless it is asked to do his/her best to perform them. This is done deliberately to explore the notion of of establishing an \emph{interpassive} relationship between the technological object (the computer displaying the score) and the performer---the pianist becomes frantically active by the `impossible' demands from the technological object which remains passive.\footnote{See \hyperlink{zizekinterpassiv}{pp. 48--49}.}

\emph{On Violence} appropriates existing music form various sources: Dieterich Buxtehude's \emph{Praeludium in G Minor, BuxWV 132}, Einst\"{u}rzende Neubauten's \emph{Autobahn} and  Tristan Wagner's \emph{Parsifal} and \emph{Tristan und Isolde} and plunders sounds like political speeches, screams and metal banging. Buxtehude's \emph{Praeludium} serves as a blueprint for the whole composition---the other derived existing music is placed within its form, and is treated and processed through its shapes and contours as well as its melodic, harmonic and counterpointal content. The live electronics part is triggered by the audio analysis of the music performed by the pianist (a microphone is placed close to the piano for real-time analysis) and by the Midi pedals. Through a combination of machine listening technology and by tracking the incoming data from the Midi pedals the computer is able to follow the score, therefore triggering different sounds and types of processing according to the structure of the composition. There are various predominant sounds coming from the live electronics part at the beginning of the composition: metaling bangs, vocal sounds (screams and fragments of political speeches some of which are heavily processed), high frequency distorted sounds and a sound reminiscent to the noise produced by a motor. The metal bangs are triggered through the onset detection of the piano signal and are stretched or shortened in real-time depending on the speed at which the pianist plays. The samples of vocal sounds and the high frequency distorted sounds are selected randomly and triggered at specific moments according to the speed of the pianists playing and are also stretched or shortened depending on the tracked tempo. Some of the vocal sounds are processed by two different types of synthesis algorithms: one of them convolves two different types of weakly nonlinear oscillators which use the sample as its external force and the other convolves one weakly nonlinear oscillator with the result of a linear predictive coding (LPC) error.\footnote{Part of the SuperCollider SLUGens library by Nick Collins.} The high frequency sounds are spectrally gated samples of guitar feedback that are passed through a distortion guitar pedal. The sound reminiscent of a motor is turned on and off through the Midi pedals and consists of a low frequency pulse wave that is distorted through an overdrive guitar pedal. The pianist controls the frequency of the pulse wave with two 3G Force sensors (accelerometers) attached to his hands, which track arm movement (as the pianist lifts his/her arms, the frequency increases and as s/he lowers them, the frequency decreases). During the first section of the composition, the pianist alternates between incessant banging of chords at a periodic rhythm and controlling the `motor' sound by lifting and lowering his/her arms. The chords that s/he plays are derived from the spectral analysis (using the PartialTracker class) of Neubauten's song \emph{Autobahn} and modified both in harmonic material and register by the Buxtehude. The pitched material of the vocal and distorted high frequency sounds as well as the `motor' sound are also modified in pitch through the Buxtehude score. As the pianist's score starts to incorporate more real-time scoring elements the content of the music also gradually starts to transform. Samples of plundered recordings of Wagner's \emph{Parsifal} and \emph{Tristan und Isolde} start to emerge, however modified in their playing rate by the mapped shapes of Buxtehude's counterpoint. The Wagner samples become more prominent as the pianist is asked to improvise and react immediately to the algorithmic score, culminating in a solo electronic solo that is produced by a synthesis algorithm emulating an organ sound and playing notes derived from a recording of the corresponding section of the Buxtehude. The performer after the electronics solo, responds by playing the next phrase of the original Buxtehude on the piano. The composition ends with the following section, were the pianist is asked to improvise freely together with an electronic part that consists of a re-synthesized version of the Prelude to the third act of \emph{Parsifal}. The synthesis algorithm used in the electronics in this section uses a dynamic bank of resonators filtering noise\footnote{I created this noise using the same algorithm I used to generate the `piano noise' in \emph{E-tudes}, however controlling its amplitude with low frequency noise, to emulate bowing.} and modified in frequency and amplitude by the extracted fundamental of the instruments in the original plundered recording, resulting in a sound that approximates a bowed string instrument. This synthesis algorithm is then used to emulate a string orchestra by programming it to perform the parts of the different instruments of the string orchestra (violins, violas, cellos, double basses) and by multiplying its results by the amount of instruments per section and by adding a slight random divination in pitch and timing for each result to imitate the sound of various instruments playing in unison. This version of the Wagner Prelude however is altered in its pitch content to match the previous section and for that reason the notes of the original are modified through the Buxtehude harmonic material. 
 
This composition is inspired by slovenian philosopher Slavoj \v{Z}i\v{z}ek's book called \emph{Violence}.\footnote{See \hyperlink{zizekviolence}{\v{Z}i\v{z}ek (2008)}.}
In this book, \v{Z}i\v{z}ek categorizes violence into two main types: subjective and objective violence. Subjective violence is clearly identifiable by an agent, for example acts of terror or crime, and it is perceived as a clear interruption of the normal state of things. On the other hand, objective violence is violence that is inherent in the social fabric and it is hard to see and experience for the advantaged classes or countries. What \v{Z}i\v{z}ek argues is that objective violence is inherent within the social ``balance'' and it is objective violence which triggers acts of subjective violence. Furthermore, \v{Z}i\v{z}ek identifies two types of objective violence: symbolic and systematic violence. Systematic violence is manifested through our economic and political systems that in order to give the idea of a normal smooth running of things, exert systematic violence on large groups of people. Symbolic violence is related to and included within systematic violence but it is specific to violence expressed through language and other symbolic systems (like music). \v{Z}i\v{z}ek goes further to argue that the forms of symbolic violence are actually based on and manifested by the symbolic systems as such.\footnote{Ibid. pp. 8--63.} \emph{On Violence} therefore attempts to explore the aesthetics of violence and reflect on the different manifestations of violence categorized by \v{Z}i\v{z}ek. It does so, not only through the type of sounds and plundered music it uses (which are suggestive of different types of violence), but by the way in which the performance itself is set up. For instance, the pianist is not only asked to be violent by the sheer force and agressiveness s/he has to execute over the instrument, but also violence is forced upon him/her by the amount of information that is thrown to him/her by technology and by the difficulty of performing all the tasks s/he is asked to. Even though at the beginning of the composition, it seems that the pianist is the one exerting subjective violence onto the piano and controlling technology---by triggering sounds through pedals and controlling the `motor' sound through the sensors---we later find out that it is technology that is controlling the pianist by flooding him/her with impossible demands. At the end of the composition, while the pianist is seemingly free (s/he is asked to improvise freely and play what ever s/he wants) we get the impression that s/he is not the one in control and actually is the victim of objective violence imposed systematically upon him/her not only by the system of performance and technology, but by the symbolic violence implicit in the music itself.

\section{\v{Z}i\v{z}ek!?}

\emph{\v{Z}i\v{z}ek!?}\footnote{See \hyperlink{portfolio}{Contents of Portfolio}: CD I (\emph{Compositions}), Track 6 and DVD III (\emph{\v{Z}i\v{z}ek!?}).} was commissioned for an event about slovenian philosopher Slavoj \v{Z}i\v{z}ek, which took place at The Sound Source, Kings Place, London, July, 2009. I was commissioned to write a live alternative soundtrack to Astra Taylor's \emph{\v{Z}i\v{z}ek!} (2005) documentary, which resulted in a \mbox{computer-mediated} performance for three improvisers playing piano, double bass and drums.\footnote{It was premiered by Alexander Hawkins (piano), Dominic Lash (double bass) and Javier Carmona (drums).} In \emph{\v{Z}i\v{z}ek!?}, each improviser has a laptop in front of them, connected through a computer network by which the improvisers receive individual written directions, timing instructions, score animations (moving graphical notation) and through headphones, each of them receives a different aural score that consists of music they have to interact with.\footnote{See DVD III (\emph{\v{Z}i\v{z}ek!?}) \tiny \textgreater \footnotesize \hspace{0pt} Documentation \tiny \textgreater \footnotesize \hspace{0pt} Performance Materials \tiny \textgreater \footnotesize \hspace{0pt} Score Parts.} The result is a real-time multimedia score that is synchronized with the film and is driven by computer programmes written in SuperCollider,\footnote{The code of the programmes that run \emph{\v{Z}i\v{z}ek!?} can be found at \href{http://github.com/freuben/Zizek}{\texttt {http://github.com/freuben/Zizek}}.} including the AlgorithmicScore class discussed previously.\footnote{See \hyperlink{algoscore}{pp. 104--109}} The improvisers receive different types of written directions through the laptop displays, which all have meanings regarding how they should react during the performance:
\begin{enumerate}
\item \emph{Silence}: Don't play (or stop playing).
\item \emph{Like} : Imitate the music coming from the headphones.
\item \emph{With} : Play together with the music coming from the headphones.
\item \emph{Free}: Free to improvise at will.
\item \emph{Solo}: Improvise a solo.
\item \emph{Unlike}: Play in opposition to the music coming from the headphones. 
\item \emph{Without}: Improvise ignoring the sound from the headphones.
\end{enumerate}
The letters of the written directions are also color coded using the three colors of a stop light: the \emph{Silence} direction is always in \emph{red} meaning they should stop playing at that point, the other directions are displayed in \emph{green} when they should play and \emph{yellow} when the \emph{Silence} direction is about to come. The performers also receive a written indication of which scene of the film is playing at a specific moment in time. The letters of these indications are in \emph{blue} while a specific scene is playing but turn \emph{pink} just before the next scene is about to start. In addition to written directions and indications, the multimedia score also includes score animations, which consist of moving graphical scores that convey some type of activity or gesture.\footnote{See \hyperlink{algoanimation}{pp. 107--108}.} Each score animation is deliberately devised for a specific instrument and is open to interpretation by the performer.\footnote{See DVD III (\emph{\v{Z}i\v{z}ek!?}) \tiny \textgreater \footnotesize \hspace{0pt} Documentation \tiny \textgreater \footnotesize \hspace{0pt} Score Animations, for the interpretation of the animations by the improvisers during the performance.} The written directions, audio score and score animations are triggered through a control structure derived from a Midi file of Johannes Ockeghem's \emph{Missa Mi-mi} (using computer-aided-composition tools described in the previous chapter).\footnote{See \hyperlink{compueraided}{pp. 109--113}.} The music that the aural score is comprised of and to which the improvisers have to react, at the same time is based on different types of analysis and processing of the audio of the film. Therefore, the aural score is not only synchronized to the film's audio, but it is derived from it. The music within the aural score is the result of different types of analysis from the speech, music and other sounds that comprise the film's soundtrack. Pitch and rhythmic material as well as frequency content and dynamics are therefore derived from the analysis of the audio signal of the soundtrack using different techniques, including tempo tracking, onset detection and spectral analysis (PartialTracker and SpearToMidi classes)\footnote{See \hyperlink{spectrack}{pp. 95--103}.} and combined/modified through the information obtained from the Ockeghem Midi file. This process generates data structures that control synthesis algorithms or triggers sampled sounds to create the final musical outcome. Moreover, the audio of the film is sometimes radically processed and used within the audio score. The result is an audio score that while retaining some characteristics of the speech and other sounds from the film, is also allusive to other styles of music as a consequence of the chosen sounds and analysis parameters.\footnote{See DVD III (\emph{\v{Z}i\v{z}ek!?}) \tiny \textgreater \footnotesize \hspace{0pt} Documentation \tiny \textgreater \footnotesize \hspace{0pt} Score Demo.} The way in which the improvisers react to the audio score also produces a musical result that is related to the characteristics of speech and other sounds in the soundtrack of the film and creates and interaction between the musicians and the live sound of the film, which they might carry into their free improvisations.

\emph{\v{Z}i\v{z}ek!?} attempts to find new ways of collaborating with musicians with a background in improvisation by using computer-mediated-performance strategies that take advantage of their strengths as musicians but at the same time working within a pre-composed structure. It does so by reshaping relationships established between composer and performer in traditional western music through technology by using a multimedia score to transfer musical ideas and intention as well as to facilitating certain types of group playing and synchronicity that otherwise would be difficult to achieve.\footnote{See \hyperlink{superscore}{pp. 39--42}, for a discussion about the possibilities of using technology to produce a multimedia score which facilitates communication with and between musicians from different backgrounds.} In addition, by giving the improvisers visual directions and aural stimulus, it is possible to direct them towards a certain type of musical behavior and sound world without limiting their creative input in the performance. Furthermore, \emph{\v{Z}i\v{z}ek!?} is devised in a way that within a fixed structure certain details may vary through improvisation, however within the constraints given by the visual and aural score. This computer-mediated-performance therefore shares certain characteristics with generative music in that the composer does not specify every detail of the final outcome but a set of possible outcomes that share similar characteristics.\footnote{See \hyperlink{realtimepos}{pp. 54--56}, for a further discussion about the relationship between improvisation and real-time computation, generative music and interactive systems.} \emph{\v{Z}i\v{z}ek!?} also explores the idea of establishing \emph{interpassive} relationships between the performers on stage and the technological objects (the laptops in front of them).\footnote{See \hyperlink{interpassiv}{pp. 46--50}.} Like \emph{On Violence}, this performance is devised in a way that the technological object remains passive (the laptops do not produce sound or activity that is apparent to the audience) while demanding from the performers to remain (hyper)active, therefore delegating their passivity on to the object and giving the semblance of reality to the illusion that they are under control, when they are actually following the demands from the laptops. Finally, \emph{\v{Z}i\v{z}ek!?} deals with the process of appropriation slightly differently from \emph{plunderphonics} or \emph{musica derivata}. First, the composer creates a new musical result by appropriating live performances as opposed to creating music through plundered recordings or by writing a score based on derived material from existing music to be realized by either classically trained musicians or mechanical instruments. It does so by appropriating the performance of musicians with a background in improvisation and by manipulating their creative output through a multimedia score. Therefore, \emph{\v{Z}i\v{z}ek!?} appropriates live performances differently than \emph{E-tudes} in that instead of plundering different types of signal from live performances to create a new composition, the process of appropriation starts by choosing living improvisers (instead of choosing  from recordings, audio and Midi signals of existing music) and manipulating/directing their creative output towards a desired musical result. Furthermore, \emph{\v{Z}i\v{z}ek!?} appropriates a film into the performance and superimposes live music on top of its original soundtrack. The live music performance at the same time interferes with the appreciation of film by trying to steal the audience's attention from it. The activity and volume (the music is meant to be louder than conventional film music, masking the audio of the film at certain points) of the improvisation therefore at some points competes with \v{Z}i\v{z}ek's own relentlessness as a speaker. At the same time however, because the live music is related to the audio of the soundtrack (the music in the aural score that the improvisers are asked react to is derived from the audio of the film), the performance amplifies \v{Z}i\v{z}ek's own hyperactivity, contributing to give the appearance that he is not as the analyst but the analysand. The result of the performance as a whole is an overload of information for the audience, which by not being able to grasp the whole content of the performance (the meaning of the concepts being discussed, the images of the documentary and performance as well as the detail of the music) has to give something up---the audience might subconsciously choose to ignore the music in order to try to concentrate on understanding what is being said or to perceive the performance as sound, rendering \v{Z}i\v{z}ek's speech as music and therefore restraining it from its meaning as language. The audience might also come to terms with the impossibility of fully grasping what is presented at them and stop trying to understand all of the different elements of the performance and give up on it entirely, becoming passive spectators of an `empty ritual'.

\section{FreuPinta}

\emph{FreuPinta}\footnote{See \hyperlink{portfolio}{Contents of Portfolio}: CD II (\emph{FreuPinta}).} is a collection of small experimental pieces that were realized in parallel to the more substantial musical projects. They are often small experiments, studies, fragments and residues of the bigger pieces that by themselves might not be considered as significant achievements but nevertheless, I consider they still have some value on their own. During the research period, I produced several of these short musical experiments in parallel to some the core computer programmes I developed, often being the testing bench of their functionality.\footnote{See \hyperlink{chapter5}{Chapter 5} for a short description of these computer programmes.} The aesthetic value of these pieces, I believe lies in that they were developed through an experimental methodology and I think also reflect my broader aesthetic concerns. Many of these short experiments are concerned with finding new interesting approaches to musical appropriation.\footnote{See \hyperlink{chapter4}{Chapter 4} for a discussion of musical strategies that deal with appropriation as a creative tool.} However the way in which they deal with appropriation may seem less sophisticated and complicated than the rest of the submitted work. However, because of the straight forward approach these short pieces deal with appropriation, the process of appropriation within them also reveals itself as more direct and apparent. Furthermore, many of these pieces were the result of documented experiments in computer programme development that aimed at finding new types of relationships between performer, composer and audience through technology.\footnote{See \hyperlink{chapter3}{Chapter 3} for a further discussion on how technology might help reshape relationships between people involved in making and experiencing music.} A considerable amount of pieces from this selection were also the result of experiments with generative, spectral and other algorithmic techniques that were implemented using real-time computation and interactive systems. \emph{FreuPinta} was also the platform were I first experimented with synthesis algorithms and physical models before implementing them in the bigger projects. My first experiments with \emph{real-time plunderphonics} and \emph{live musica derivata} are also documented as part of this set of short pieces. 

Some of the pieces that comprise the \emph{FreuPinta} collection are divided into different series of works. The \emph{simulation series} comprises of four short pieces that deal with appropriation by attempting to simulate human performances of existing music through technology. Technology therefore brings the tools to emulate an `artificial' performance of a recording of human musicians performing different types of music. The process of simulating human performance however is not with the purpose of reproducing the original but of transcribing the music to be realized by `virtual' performers. This is done by analyzing the recording of the original performance using SpearToMidi\footnote{See \hyperlink{spearmidi}{pp. 101--103}.} using different parameters to generate a `virtual' score that may be performed by either synthesis algorithms the emulate acoustic instruments or by mechanical instruments. For instance, \emph{Simulation No. 1} is a `virtual' performance of the \emph{Overture} to Handel's \emph{Solomon} oratorio realized by the computer using different `intrumental' sounds, therefore `re-orchestrating' the original version. The process of transcription from the version performed by humans to the computer performance also introduces new sounds, pitches and rhythms that are idiosyncratic to the computer medium. \emph{Simulation No. 2} is a simulation of Radiohead's \emph{Pyramid Song} which derives data from the original recording to be performed by a mechanical piano. \emph{Simulation No. 3} and \emph{Simulation No. 4} are both material for the electronic part of \emph{On Violence} that in their present form did not make it to the final version. They are simulations that use synthesis algorithms (which emulate an organ and a string orchestra respectively) to perform music derived from the computer's analysis of short fragments of recordings of Wagner (Prelude to third act of \emph{Parsifal}) and Buxtehude (\emph{Praeludium in G Minor}). The \emph{occupation series} comprises of three short pieces that all deal with appropriation in the same way: they appropriate a recording which is not processed or transformed at all and is left in its original form, however adding new sounds on top of the recording that are mixed with the original.\footnote{This series therefore uses a strategy similar to Duchamp's notion of \emph{assisted readymades}. See \hyperlink{lhooq}{pp. 65--66}.} The added sounds at the same time are also derived from the analysis of the recording itself and are the result of recordings of `performances' using real-time computation---I used PartialTracker\footnote{See \hyperlink{partrack}{p. 97}.} and onset detection algorithms with different varying parameters that I had control over during the performance to trigger the new sounds, which were recorded. The result of the added sounds also often re-contextualizes the meaning of the appropriated musical source. \emph{Occupation No.1} is a personal small homage to Stockhausen's \emph{Hymnen}---it appropriates the recording of the national anthem of my country (which is missing from Stockhausen's piece) and adds \emph{synthrumentized} piano chords over it. \emph{Occupation No.2} is a study for \emph{\v{Z}i\v{z}ek!?} which adds instrumental sounds (sampled guitar and percussion instruments) and sinewaves to the recording of a lecture by \v{Z}i\v{z}ek---the instrumental sounds at the same time are derived from \v{Z}i\v{z}ek's speech. \emph{Occupation No.3} appropriates a fragment of Giovanni Palestrina's \emph{Missa Papae Marcelli} and adds on top a `virtual' jazz trio that `swings' along with the original. The \emph{transgression series} uses a different approach to the process of appropriation: it plunders data from the original source and then rearranges and reshuffles its components, re-imagining the original composition by transgressing its symbolic space. \emph{Transgression No. 1} re-imagines the first movement of Mozart's \emph{Sonata in C KV 545, I}, while \emph{Transgression No. 2} envisions an alternative condition to the third movement of Bartok's \emph{Suite, Op.14, Sz62}. The remaining pieces in the \emph{FreuPinta} collection include more experiments controlling synthesis algorithms with data derived from spectral analysis of existing music (\emph{Chorus I}, \emph{Chorus II} and \emph{Agnus Dei}), my first experiments with the notions of \emph{real-time plunderphonics} and \emph{live plunderphonics} (\emph{Canzione Piramide} and \emph{La pieza del minuto}), experiments with interactive systems for live improvisation (\emph{Pianpeta}, Horatio No.3-5) and mapping data from robotic movement to music (\emph{Walking Head}).

\section{Improvisations}

The improvisations presented as part of the submitted work\footnote{See \hyperlink{portfolio}{Contents of Portfolio}: CD III (\emph{Improvisations}) and DVD IV (\emph{Improvisations}).} are the outcome of the work I have done in collaboration with other improvisers using the computer environment I developed in SuperCollider for live improvisation.\footnote{See \hyperlink{improvprog}{p. 113}.} The recordings of these improvisations are the result of several concerts and rehearsals that took place during the period of research. The submitted work includes fragments of several recordings:
\begin{enumerate} 
\item \emph{Live@ICA}: concert of improvised music for a live performance art event called The Scuttler presented by boyleANDshaw, which took place in June, 2010 at Institute of Contemporary Art (ICA) in London. Free improvisation for live electronics, percussion, processed voice and guitar.\footnote{The performance featured Javier Carmona on percussion and Adam de la Cour on processed voice/guitar.}
\item \emph{To Hot to Handel}: I was asked to present a work at the Handel House Museum. The museum is located on the upper floors of 25 and 23 Brook Street in London. 25 Brook Street was the residence of George Frideric Handel between 1723 and 1759. 23 Brook Street, where the concert took place was the home of Jimi Hendrix 1968-1969. I decided to present a site-specific event that combined the music of both Handel and Hendrix simultaneously within a structured improvisation for live electronics, voice, guitar, harpsichord, drums, double bass and narrator/conductor.\footnote{The performance featured Adam de la Cour (voice/guitar), Alexander Hawkins (harpsichord), Javier Carmona (drums), Dominic Lash (double bass) and Steve Potter (voice/conductor).}
\item \emph{Horatio Oratorio}: Horatio Oratorio is a performance and sound installation that employs archival sound sources, including some of the first recorded utterances and music. The music was composed and improvised by Aleksander Kolkowski and myself. The improvisations feature Stroh violin (Kolkowski) and live electronics (Reuben).\footnote{See \hyperlink{reuben}{Reuben and Kolkowski (2008)}.}
\item \emph{Live@Javier's}: Free improvisation for live electronics, drums and saxophone.\footnote{Featuring Javier Carmona on drums and Paulina Owczarek on saxophone.}
\item \emph{Mowgli@Cafe Oto}: Concert at Cafe Oto, Dalston, London. Structured improvisation for live electronics, voice/guitar, drums, double bass.\footnote{The performance featured Adam de la Cour (voice/guitar), Javier Carmona (drums) and Dominic Lash (double bass).}
\end{enumerate}

\indent

Conclusion?

\label{ch:compositions}